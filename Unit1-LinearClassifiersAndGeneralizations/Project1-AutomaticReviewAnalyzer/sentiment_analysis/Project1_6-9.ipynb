{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import project1 as p1\n",
    "import utils\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Automative review analyzer\n",
    "\n",
    "> Now that you have verified the correctness of your implementations, you are ready to tackle the main task of this project: building a classifier that labels reviews as positive or negative using text-based features and the linear classifiers that you implemented in the previous section!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "\n",
    "> The data consists of several reviews, each of which has been labeled with  âˆ’1  or  +1 , corresponding to a negative or positive review, respectively. The original data has been split into four files:\n",
    "\n",
    "- reviews_train.tsv (4000 examples)\n",
    "- reviews_validation.tsv (500 examples)\n",
    "- reviews_test.tsv (500 examples)\n",
    "\n",
    "> To get a feel for how the data looks, we suggest first opening the files with a text editor, spreadsheet program, or other scientific software package (like pandas)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translating reviews to feature vectors\n",
    "\n",
    "> We will convert review texts into feature vectors using a bag of words approach. We start by compiling all the words that appear in a training set of reviews into a dictionary , thereby producing a list of  ğ‘‘  unique words.\n",
    "\n",
    "\n",
    "> We can then transform each of the reviews into a feature vector of length  ğ‘‘  by setting the  ğ‘–th  coordinate of the feature vector to  1  if the  ğ‘–th  word in the dictionary appears in the review, or  0 otherwise. For instance, consider two simple documents â€œMary loves apples\" and â€œRed apples\". In this case, the dictionary is the set  {Mary;loves;apples;red} , and the documents are represented as  (1;1;1;0)  and  (0;0;1;1) .\n",
    "\n",
    "> A bag of words model can be easily expanded to include phrases of length  ğ‘š . A unigram model is the case for which  ğ‘š=1 . In the example, the unigram dictionary would be  (Mary;loves;apples;red) . In the bigram case,  ğ‘š=2 , the dictionary is  (Mary loves;loves apples;Red apples) , and representations for each sample are  (1;1;0),(0;0;1) . In this section, you will only use the unigram word features. These functions are already implemented for you in the bag of words function.\n",
    "In utils.py, we have supplied you with the load data function, which can be used to read the .tsv files and returns the labels and texts. We have also supplied you with the bag_of_words function in project1.py, which takes the raw data and returns dictionary of unigram words. The resulting dictionary is an input to extract_bow_feature_vectors which computes a feature matrix of ones and zeros that can be used as the input for the classification algorithms. Using the feature matrix and your implementation of learning algorithms from before, you will be able to compute  ğœƒ and  ğœƒ0 .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Classification and Accuracy\n",
    "\n",
    "> Now we need a way to actually use our model to classify the data points. In this section, you will implement a way to classify the data points using your model parameters, and then measure the accuracy of your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "> Implement a classification function that uses  ğœƒ  and  ğœƒ0  to classify a set of data points. You are given the feature matrix,  ğœƒ , and  ğœƒ0  as defined in previous sections. This function should return a numpy array of -1s and 1s. If a prediction is greater than zero, it should be considered a positive classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(feature_matrix, theta, theta_0):\n",
    "    \"\"\"\n",
    "    A classification function that uses theta and theta_0 to classify a set of\n",
    "    data points.\n",
    "\n",
    "    Args:\n",
    "        feature_matrix - A numpy matrix describing the given data. Each row\n",
    "            represents a single data point.\n",
    "                theta - A numpy array describing the linear classifier.\n",
    "        theta - A numpy array describing the linear classifier.\n",
    "        theta_0 - A real valued number representing the offset parameter.\n",
    "\n",
    "    Returns: A numpy array of 1s and -1s where the kth element of the array is\n",
    "    the predicted classification of the kth row of the feature matrix using the\n",
    "    given theta and theta_0. If a prediction is GREATER THAN zero, it should\n",
    "    be considered a positive classification.\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    predictions = []\n",
    "    \n",
    "    for feature_vector in feature_matrix:\n",
    "        prediction = np.dot(theta, feature_vector) + theta_0\n",
    "        if prediction >= 0:\n",
    "            predictions.append(1)\n",
    "        else:\n",
    "            predictions.append(-1)\n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.array([])\n",
    "res = np.array([])\n",
    "for i in range(10):\n",
    "    print(i)\n",
    "    tmp = np.append(arr, [i])\n",
    "    res = np.append(res, tmp)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = []\n",
    "for i in range( 10 ):\n",
    "    x.append( i )\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, -1]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = []\n",
    "y.append(1)\n",
    "y.append(-1)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "\n",
    "> We have supplied you with an accuracy function:\n",
    "\n",
    "```python\n",
    "def accuracy(preds, targets):\n",
    "    \"\"\"\n",
    "    Given length-N vectors containing predicted and target labels,\n",
    "    returns the percentage and number of correct predictions.\n",
    "    \"\"\"\n",
    "    return (preds == targets).mean()\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "> The accuracy function takes a numpy array of predicted labels and a numpy array of actual labels and returns the prediction accuracy. You should use this function along with the functions that you have implemented thus far in order to implement classifier_accuracy.\n",
    "\n",
    "> The classifier_accuracy function should take 6 arguments:\n",
    "\n",
    "\n",
    "- a classifier function that, itself, takes arguments (feature_matrix, labels, **kwargs)\n",
    "\n",
    "- the training feature matrix\n",
    "\n",
    "- the validation feature matrix\n",
    "\n",
    "- the training labels\n",
    "\n",
    "- the valiation labels\n",
    "\n",
    "- a **kwargs argument to be passed to the classifier function\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ–¹é‡\n",
    "\n",
    "1. Training phase\n",
    "    - todo:\n",
    "        - training datasetã‚’ä½¿ã£ã¦`classifier(feature_matrix, labels, **kwargs)` functionã§(theta, theta_0)ã‚’ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹\n",
    "    - input:\n",
    "        - `training feature matrix`\n",
    "        - `training labels`\n",
    "    - output: \n",
    "        - `theta`, `theta`, `theta_0`\n",
    "2. Validation phase\n",
    "    - todo:\n",
    "        - å‰å•ã§å®Ÿè£…ã—ãŸ`classify(feature_matrix, theta, theta_0)`ã‚’ä½¿ã£ã¦ã€ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿(theta, theta_0)ã‚ˆã‚Šãƒ©ãƒ™ãƒ«ä»˜ã‘(+1,-1)ã‚’è¡Œã†\n",
    "    - input1:\n",
    "        - `traing feature matrix`\n",
    "        - `(theta, theta_0)`\n",
    "    - output1;\n",
    "        - traing dataset predictions\n",
    "    - input2:\n",
    "        - `validation feature matrix`\n",
    "        - `(theta, theta_0)`\n",
    "    - output2;\n",
    "        - validation dataset predictions\n",
    "3. Compute the classification accuracy phase\n",
    "    - todo:\n",
    "        - `accuracy(preds, targets)`ã‚’ä½¿ã£ã¦traning/validation predictionsã¨traning/validation labelsã‚’æ¯”è¼ƒã—ã¦classification accuracyã‚’ç®—å‡ºã™ã‚‹\n",
    "    - input1:\n",
    "        - training predictions\n",
    "        - training labels\n",
    "    - output1:\n",
    "        - training accuracy\n",
    "    - input2:\n",
    "        - validation predictions\n",
    "        - validation labels\n",
    "    - output2:\n",
    "        - validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_accuracy(\n",
    "        classifier,\n",
    "        train_feature_matrix,\n",
    "        val_feature_matrix,\n",
    "        train_labels,\n",
    "        val_labels,\n",
    "        **kwargs):\n",
    "    \"\"\"\n",
    "    Trains a linear classifier and computes accuracy.\n",
    "    The classifier is trained on the train data. The classifier's\n",
    "    accuracy on the train and validation data is then returned.\n",
    "\n",
    "    Args:\n",
    "        classifier - A classifier function that takes arguments\n",
    "            (feature matrix, labels, **kwargs) and returns (theta, theta_0)\n",
    "        train_feature_matrix - A numpy matrix describing the training\n",
    "            data. Each row represents a single data point.\n",
    "        val_feature_matrix - A numpy matrix describing the training\n",
    "            data. Each row represents a single data point.\n",
    "        train_labels - A numpy array where the kth element of the array\n",
    "            is the correct classification of the kth row of the training\n",
    "            feature matrix.\n",
    "        val_labels - A numpy array where the kth element of the array\n",
    "            is the correct classification of the kth row of the validation\n",
    "            feature matrix.\n",
    "        **kwargs - Additional named arguments to pass to the classifier\n",
    "            (e.g. T or L)\n",
    "\n",
    "    Returns: A tuple in which the first element is the (scalar) accuracy of the\n",
    "    trained classifier on the training data and the second element is the\n",
    "    accuracy of the trained classifier on the validation data.\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    \n",
    "    ## 1. Training phase ##\n",
    "    parameters = classifier(train_feature_matrix, train_labels, **kwargs)\n",
    "    \n",
    "    ## 2. Validation phase\n",
    "    training_predections = classify(train_feature_matrix, parameters[0], parameters[1])\n",
    "    validation_predecions = classify(val_feature_matrix, parameters[0], parameters[1])\n",
    "    \n",
    "    ## 3. Compute the classification accuracy phase\n",
    "    traing_accuracy = p1.accuracy(training_predections, train_labels)\n",
    "    validation_accuracy = p1.accuracy(validation_predecions, val_labels)\n",
    "    \n",
    "    return (traing_accuracy, validation_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### A note on the classifier() call in classifier_accuracy: Python and first-class functions\n",
    "\n",
    "> For folks new to Python or functional programming: In Python, one can store a reference to a function in a variable, and pass that function reference as an argument. Then one can later call the function by using the variable name just as though it were the function name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False,  True])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a reference to an example function and store it in a variable.\n",
    "# This just makes an alias for the function.\n",
    "foo = np.greater\n",
    "# And some arguments for it.\n",
    "a = np.array([1, 2, 3])\n",
    "b = np.array([2, 2, 2])\n",
    "# Now call the function.\n",
    "foo(a, b)\n",
    "# Result is array([False, False,  True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Parameter Tuning\n",
    "\n",
    "> You finally have your algorithms up and running, and a way to measure performance! But, it's still unclear what values the hyperparameters like  ğ‘‡  and  ğœ†  should have. In this section, you'll tune these hyperparameters to maximize the performance of each model.\n",
    "\n",
    "---\n",
    "\n",
    "> One way to tune your hyperparameters for any given Machine Learning algorithm is to perform a grid search over all the possible combinations of values. If your hyperparameters can be any real number, you will need to limit the search to some finite set of possible values for each hyperparameter. For efficiency reasons, often you might want to tune one individual parameter, keeping all others constant, and then move onto the next one; Compared to a full grid search there are many fewer possible combinations to check, and this is what you'll be doing for the questions below.\n",
    "\n",
    "> In main.py uncomment Problem 8 to run the staff-provided tuning algorithm from utils.py. For the purposes of this assignment, please try the following values for  ğ‘‡ : [1, 5, 10, 15, 25, 50] and the following values for  ğœ†  [0.001, 0.01, 0.1, 1, 10]. For pegasos algorithm, first fix  ğœ†=0.01  to tune  ğ‘‡ , and then use the best  ğ‘‡  to tune  ğœ†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword\n",
    "\n",
    "- hyperparameter\n",
    "- grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance After Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result\n",
    "\n",
    "- best method: `Pagasos`\n",
    "\n",
    "```\n",
    "perceptron valid: [(1, 0.758), (5, 0.72), (10, 0.716), (15, 0.778), (25, 0.794), (50, 0.79)]\n",
    "best = 0.7940, T=25.0000\n",
    "avg perceptron valid: [(1, 0.794), (5, 0.792), (10, 0.798), (15, 0.798), (25, 0.8), (50, 0.796)]\n",
    "best = 0.8000, T=25.0000\n",
    "Pegasos valid: tune T [(1, 0.7), (5, 0.782), (10, 0.794), (15, 0.806), (25, 0.804), (50, 0.806)]\n",
    "best = 0.8060, T=15.0000\n",
    "Pegasos valid: tune L [(0.001, 0.79), (0.01, 0.806), (0.1, 0.752), (1, 0.594), (10, 0.518)]\n",
    "best = 0.8060, L=0.0100\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy on the test set\n",
    "\n",
    "> After you have chosen your best method (perceptron, average perceptron or Pegasos) and parameters, use this classifier to compute testing accuracy on the test set. \n",
    "\n",
    "> We have supplied the feature matrix and labels in main.py as test_bow_features and test_labels.\n",
    "\n",
    "> Note: In practice the validation set is used for tuning hyperparameters while a heldout test set is the final benchmark used to compare disparate models that have already been tuned. You may notice that your results using a validation set don't always align with those of the test set, and this is to be expected.\n",
    "\n",
    "\n",
    "```python\n",
    "T=25\n",
    "L=0.0100\n",
    "\n",
    "# Your code here\n",
    "avg_peg_train_accuracy, avg_peg_test_accuracy = \\\n",
    "   p1.classifier_accuracy(p1.pegasos, train_bow_features,test_bow_features,train_labels,test_labels,T=T,L=L)\n",
    "print(\"{:50} {:.4f}\".format(\"Training accuracy for Pegasos:\", avg_peg_train_accuracy))\n",
    "print(\"{:50} {:.4f}\".format(\"Test accuracy for Pegasos:\", avg_peg_test_accuracy))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result\n",
    "\n",
    "```\n",
    "Training accuracy for Pegasos:                     0.9195\n",
    "Test accuracy for Pegasos:                         0.8020\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The most explanatory unigrams\n",
    "\n",
    "> According to the largest weights (i.e. individual  ğ‘–  values in your vector), you can find out which unigrams were the most impactful ones in predicting positive labels. Uncomment the relevant part in main.py to call utils.most_explanatory_word.\n",
    "\n",
    "> Report the top ten most explanatory word features for positive classification below:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### åˆ†æ\n",
    "\n",
    "- thetaã®è¦ç´ ã™ã‚‹ã¯`13234`å€‹\n",
    "- typeã¯numpyã®array\n",
    "\n",
    "```\n",
    "theta dim: (13234,)\n",
    "theta type: <class 'numpy.ndarray'>\n",
    "```\n",
    "- thetaã®ä¸­èº«ã®ãƒˆãƒƒãƒ—10\n",
    "    - thetaã‚’é™é †ã«ã‚½ãƒ¼ãƒˆ\n",
    "\n",
    "```\n",
    "0.5903715521713366\n",
    "\n",
    "0.5685059656607246\n",
    "\n",
    "0.44734183045507536\n",
    "\n",
    "0.40064895432564757\n",
    "\n",
    "0.38411827540030485\n",
    "\n",
    "0.36204152120051414\n",
    "\n",
    "0.3599248573066697\n",
    "\n",
    "0.35073693119923643\n",
    "\n",
    "0.32758790048448905\n",
    "\n",
    "0.3274736683441225\n",
    "```\n",
    "\n",
    "- wordlist[list]ã®ã‚µã‚¤ã‚º\n",
    "    - thetaã®æ•°ã¨åŒã˜\n",
    "\n",
    "```\n",
    "13234\n",
    "```\n",
    "\n",
    "- `most_explanatory_word(theta, wordlist)`ã‚’åˆ†æ\n",
    "    - thetaã®å„è¦ç´ ï¼ˆé‡ã¿ï¼‰ã¨wordã‚’zipã§ã²ã‚‚ä»˜ã‘ã—ã¦ã„ã‚‹\n",
    "    - é™é †ã‚½ãƒ¼ãƒˆã¯ã“ã®é–¢æ•°å†…ã§è¡Œã£ã¦ã„ã‚‹ã®ã§ã€ã“ã®é–¢æ•°ã«æ¸¡ã™å‰ã«thetaã‚’ã‚½ãƒ¼ãƒˆã—ã¦ã—ã¾ã†ã¨ã€wordã¨ã®ã²ã‚‚ä»˜ã‘ãŒé–“é•ã£ãŸçŠ¶æ…‹ã«ãªã£ã¦ã—ã¾ã†ã€‚\n",
    "    - wordlistã¯13234å€‹ã‚ã‚‹ã®ã§ã€thetaã‚‚ãã®ã¾ã¾ã®è¦ç´ æ•°ã§æ¸¡ã™\n",
    "\n",
    "```\n",
    "[word for (theta_i, word) in sorted(zip(theta, wordlist))[::-1]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([2,1,10,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  1,  2, 10])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(a, kind='quicksort')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139985953269360"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[::-1].sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10,  2,  1, -1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139985953269360"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### most_explanatory_wordé–¢æ•°å†…ã®zipã®æŒ™å‹•åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_dummy = np.random.randint(1,10,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 7, 9, 3, 5])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist = ['apple', 'banana', 'grape', 'orange', 'peach']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['apple', 'banana', 'grape', 'orange', 'peach']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### zipé–¢æ•°ã‚’ä½¿ã£ã¦ã¿ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped = zip(theta_dummy, wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((2, 'apple'), (7, 'banana'), (9, 'grape'), (3, 'orange'), (5, 'peach'))\n"
     ]
    }
   ],
   "source": [
    "print(tuple(zipped))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### zipã—ãŸã‚‚ã®ã‚’ã‚½ãƒ¼ãƒˆã—ã¦ã¿ã‚‹\n",
    "\n",
    "Note: [What is the difference between sorted(list) vs list.sort()?](https://stackoverflow.com/questions/22442378/what-is-the-difference-between-sortedlist-vs-list-sort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_zipped = sorted(zip(theta_dummy, wordlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 'apple'), (3, 'orange'), (5, 'peach'), (7, 'banana'), (9, 'grape')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_zipped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### zipã—ãŸã‚‚ã®ã‚’é™é †ã§ã‚½ãƒ¼ãƒˆã—ã¦ã¿ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_zipped = sorted(zip(theta_dummy, wordlist))[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9, 'grape'), (7, 'banana'), (5, 'peach'), (3, 'orange'), (2, 'apple')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_zipped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### zipã™ã‚‹ã‚‚ã®åŒå£«ã®è¦ç´ æ•°ãŒåŒã˜ã§ãªã„å ´åˆ\n",
    "\n",
    "- #theta < #wordlist\n",
    "- çµæœ: å°‘ãªã„è¦ç´ æ•°ã«åˆã‚ã›ã¦zipã•ã‚Œã‚‹ã®ã§æƒ…å ±è½ã¡ãŒç™ºç”Ÿã™ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_dummy2 = np.random.randint(1,10,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_dummy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped2 = zip(theta_dummy2, wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((3, 'apple'), (2, 'banana'))\n"
     ]
    }
   ],
   "source": [
    "print(tuple(zipped2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ–¹é‡\n",
    "\n",
    "- best_thetaã«ã¯train dataã¨pegasosã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§å‡ºã—ãŸthetaã‚’ãã®ã¾ã¾ä»£å…¥\n",
    "    - ãã®ã¾ã¾ã¨ã¯ã€ã‚½ãƒ¼ãƒˆã‚„çµã‚Šè¾¼ã¿ãªã©ã¯ä¸€åˆ‡è¡Œã‚ãªã„ã¨ã„ã†ã“ã¨\n",
    "- pegasosã§data setã‚’å›ã—ã¦parameterã‚’æŠ½å‡ºã™ã‚‹ã¨ã„ã†é–¢æ•°ãŒãªã„ã®ã§ã€`classifier_accuracy` @main.pyã‚’çœŸä¼¼ã¦ã€`classifier_parameter`ã‚’main.pyã«ä½œæˆã™ã‚‹\n",
    "- ã‚¤ãƒ¡ãƒ¼ã‚¸:\n",
    "    1. text(review) -> bow_features(1,0ã§æ§‹æˆã•ã‚Œã‚‹ãƒ™ã‚¯ãƒˆãƒ«)ã«å¤‰æ›ã€‚æ–‡å­—ã‚’æ•°å€¤åŒ–ã™ã‚‹ã“ã¨ã§ã€ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®è¨ˆç®—ãŒã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ã€‚\n",
    "    2. bow_featuresã®test datasetã§ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ç”¨ã„ã¦ã€parameteræœ€é©åŒ–ã‚’è¡Œã†ã€‚ã¤ã¾ã‚Š,æœ€é©åŒ–ã•ã‚ŒãŸthetaã¨theta_0ã‚’æ±‚ã‚ã‚‹\n",
    "    3. æ±‚ã‚ãŸthetaã¯bowã®wordlistã®è¦ç´ æ•°ã¨åŒã˜è¦ç´ æ•°ã‚’æŒã¤ãƒ™ã‚¯ãƒˆãƒ«ã¨ãªã‚‹ã€‚ã“ã®å„è¦ç´ ã¯wordlistã®å„wordã®é‡ã¿ã«ãªã‚‹ã€‚ãã®ãŸã‚ã€ã‚‚ã†ä¸€åº¦ã“ã®thetaã‚’wordlistã¨ç´ä»˜ã‘ã‚’è¡Œã†ã€‚(é‡ã¿:word)ã¨ã„ã†tupleã®é›†åˆã‚’ä½œæˆã™ã‚‹ã€‚ãã—ã¦ã€é‡ã¿ã®é™é †ã§ã®ã‚½ãƒ¼ãƒˆã‚‚è¡Œã†ã€‚ï¼ˆã“ã‚ŒãŒ`most_explanatory_word`é–¢æ•°å†…ã§è¡Œã£ã¦ã„ã‚‹ã“ã¨ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### çµæœ\n",
    "\n",
    "- listupã•ã‚ŒãŸå˜èªã¯åˆã£ã¦ã„ãŸã€‚ã—ã‹ã—ã€å¾ŒåŠã®é †ä½ã¯é–“é•ã£ã¦ã„ãŸã®ã§ã€ã‚„ã¯ã‚Špegasosã®å®Ÿè£…ã«ã©ã“ã‹å•é¡ŒãŒã‚ã‚‹ã¿ãŸã„\n",
    "\n",
    "```\n",
    "Most Explanatory Word Features\n",
    "['delicious', 'great', '!', 'best', 'perfect', 'loves', 'glad', 'wonderful', 'quickly', 'love']\n",
    "```\n",
    "\n",
    "- ã¡ãªã¿ã«ã€æ˜‡é †ã‚½ãƒ¼ãƒˆã«ã™ã‚Œã°worst top10ã«ãªã‚‹\n",
    "\n",
    "```python\n",
    "return [word for (theta_i, word) in sorted(zip(theta, wordlist))]\n",
    "```\n",
    "\n",
    "```\n",
    "['disappointed', 'however', 'bad', 'not', 'but', 'unfortunately', 'awful', '\\$', 'ok', 'money']\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Feature Engineering\n",
    "\n",
    ">Frequently, the way the data is represented can have a significant impact on the performance of a machine learning method. Try to improve the performance of your best classifier by using different features. In this problem, we will practice two simple variants of the bag of words (BoW) representation.\n",
    "\n",
    "---\n",
    "\n",
    "## Remove Stop Words\n",
    "\n",
    ">Try to implement stop words removal in your feature engineering code. Specifically, load the file stopwords.txt, remove the words in the file from your dictionary, and use features constructed from the new dictionary to train your model and make predictions.\n",
    "\n",
    ">Compare your result in the testing data on Pegasos algorithm using  ğ‘‡=25  and  ğ¿=0.01  when you remove the words in stopwords.txt from your dictionary.\n",
    "\n",
    ">Hint: Instead of replacing the feature matrix with zero columns on stop words, you can modify the bag_of_words function to prevent adding stopwords to the dictionary\n",
    "\n",
    ">Accuracy on the test set using the original dictionary: 0.8020\n",
    ">Accuracy on the test set using the dictionary with stop words removed:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ–¹é‡\n",
    "\n",
    "- `stopwords.txt`ã‚’loadã—ã¦liståŒ–@main.py\n",
    "- `bag_of_words_remove_stop_words(train_texts, stopwords_data)`é–¢æ•°ã‚’project1.pyã«ä½œæˆ\n",
    "    - liståŒ–ã—ãŸstopwordsã‚’å¼•æ•°ã§æ¸¡ã›ã‚‹ã‚ˆã†ã«ã™ã‚‹\n",
    "    - wordã‚’è¿½åŠ ã™ã‚‹éš›ã«ã€stopwordsã‚’å‚ç…§ã—ã¦ã€wordãŒstopwordsã«å«ã¾ã‚Œã‚‹ã®ã§ã‚ã‚Œã°è¿½åŠ ã—ãªã„ã‚ˆã†ã«ã™ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [\n",
    "    'i',\n",
    "    'me',\n",
    "    'my',\n",
    "    'myself',\n",
    "    'we',\n",
    "    'our',\n",
    "    'ours',\n",
    "    'ourselves',\n",
    "    'you',\n",
    "    'your',\n",
    "    'yours',\n",
    "    'yourself',\n",
    "    'yourselves',\n",
    "    'he'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = [\n",
    "    'apple',\n",
    "    'my',\n",
    "    'orange'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word apple\n",
      "stop word my\n",
      "word orange\n"
     ]
    }
   ],
   "source": [
    "for w in word_list:\n",
    "    if w not in stopwords:\n",
    "        print('word ' + w)\n",
    "    else:\n",
    "        print('stop word ' + w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### çµæœ\n",
    "\n",
    "- original dictionaryã«æ¯”ã¹ã¦ã€dictionary with stop words removedã ã¨ç¢ºã‹ã«ç²¾åº¦ãŒä¸ŠãŒã£ãŸ\n",
    "- ã—ã‹ã—ã€ä¸æ­£è§£ã€‚ã‚„ã¯ã‚Špegasosã®å®Ÿè£…ã«å•é¡ŒãŒã‚ã‚‹ã‚ˆã†ã€‚\n",
    "\n",
    "```shell\n",
    "Training accuracy for Pegasos:                     0.9150\n",
    "Test accuracy for Pegasos:                         0.8100\n",
    "13108\n",
    "Most Explanatory Word Features\n",
    "['delicious', 'great', 'loves', '!', 'best', 'perfect', 'excellent', 'wonderful', 'favorite', 'love']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_6.86x)",
   "language": "python",
   "name": "conda_6.86x"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
