{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import project1 as p1\n",
    "import utils\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Automative review analyzer\n",
    "\n",
    "> Now that you have verified the correctness of your implementations, you are ready to tackle the main task of this project: building a classifier that labels reviews as positive or negative using text-based features and the linear classifiers that you implemented in the previous section!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "\n",
    "> The data consists of several reviews, each of which has been labeled with  −1  or  +1 , corresponding to a negative or positive review, respectively. The original data has been split into four files:\n",
    "\n",
    "- reviews_train.tsv (4000 examples)\n",
    "- reviews_validation.tsv (500 examples)\n",
    "- reviews_test.tsv (500 examples)\n",
    "\n",
    "> To get a feel for how the data looks, we suggest first opening the files with a text editor, spreadsheet program, or other scientific software package (like pandas)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translating reviews to feature vectors\n",
    "\n",
    "> We will convert review texts into feature vectors using a bag of words approach. We start by compiling all the words that appear in a training set of reviews into a dictionary , thereby producing a list of  𝑑  unique words.\n",
    "\n",
    "\n",
    "> We can then transform each of the reviews into a feature vector of length  𝑑  by setting the  𝑖th  coordinate of the feature vector to  1  if the  𝑖th  word in the dictionary appears in the review, or  0 otherwise. For instance, consider two simple documents “Mary loves apples\" and “Red apples\". In this case, the dictionary is the set  {Mary;loves;apples;red} , and the documents are represented as  (1;1;1;0)  and  (0;0;1;1) .\n",
    "\n",
    "> A bag of words model can be easily expanded to include phrases of length  𝑚 . A unigram model is the case for which  𝑚=1 . In the example, the unigram dictionary would be  (Mary;loves;apples;red) . In the bigram case,  𝑚=2 , the dictionary is  (Mary loves;loves apples;Red apples) , and representations for each sample are  (1;1;0),(0;0;1) . In this section, you will only use the unigram word features. These functions are already implemented for you in the bag of words function.\n",
    "In utils.py, we have supplied you with the load data function, which can be used to read the .tsv files and returns the labels and texts. We have also supplied you with the bag_of_words function in project1.py, which takes the raw data and returns dictionary of unigram words. The resulting dictionary is an input to extract_bow_feature_vectors which computes a feature matrix of ones and zeros that can be used as the input for the classification algorithms. Using the feature matrix and your implementation of learning algorithms from before, you will be able to compute  𝜃 and  𝜃0 .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Classification and Accuracy\n",
    "\n",
    "> Now we need a way to actually use our model to classify the data points. In this section, you will implement a way to classify the data points using your model parameters, and then measure the accuracy of your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "> Implement a classification function that uses  𝜃  and  𝜃0  to classify a set of data points. You are given the feature matrix,  𝜃 , and  𝜃0  as defined in previous sections. This function should return a numpy array of -1s and 1s. If a prediction is greater than zero, it should be considered a positive classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(feature_matrix, theta, theta_0):\n",
    "    \"\"\"\n",
    "    A classification function that uses theta and theta_0 to classify a set of\n",
    "    data points.\n",
    "\n",
    "    Args:\n",
    "        feature_matrix - A numpy matrix describing the given data. Each row\n",
    "            represents a single data point.\n",
    "                theta - A numpy array describing the linear classifier.\n",
    "        theta - A numpy array describing the linear classifier.\n",
    "        theta_0 - A real valued number representing the offset parameter.\n",
    "\n",
    "    Returns: A numpy array of 1s and -1s where the kth element of the array is\n",
    "    the predicted classification of the kth row of the feature matrix using the\n",
    "    given theta and theta_0. If a prediction is GREATER THAN zero, it should\n",
    "    be considered a positive classification.\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    predictions = []\n",
    "    \n",
    "    for feature_vector in feature_matrix:\n",
    "        prediction = np.dot(theta, feature_vector) + theta_0\n",
    "        if prediction >= 0:\n",
    "            predictions.append(1)\n",
    "        else:\n",
    "            predictions.append(-1)\n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.array([])\n",
    "res = np.array([])\n",
    "for i in range(10):\n",
    "    print(i)\n",
    "    tmp = np.append(arr, [i])\n",
    "    res = np.append(res, tmp)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = []\n",
    "for i in range( 10 ):\n",
    "    x.append( i )\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, -1]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = []\n",
    "y.append(1)\n",
    "y.append(-1)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "\n",
    "> We have supplied you with an accuracy function:\n",
    "\n",
    "```python\n",
    "def accuracy(preds, targets):\n",
    "    \"\"\"\n",
    "    Given length-N vectors containing predicted and target labels,\n",
    "    returns the percentage and number of correct predictions.\n",
    "    \"\"\"\n",
    "    return (preds == targets).mean()\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "> The accuracy function takes a numpy array of predicted labels and a numpy array of actual labels and returns the prediction accuracy. You should use this function along with the functions that you have implemented thus far in order to implement classifier_accuracy.\n",
    "\n",
    "> The classifier_accuracy function should take 6 arguments:\n",
    "\n",
    "\n",
    "- a classifier function that, itself, takes arguments (feature_matrix, labels, **kwargs)\n",
    "\n",
    "- the training feature matrix\n",
    "\n",
    "- the validation feature matrix\n",
    "\n",
    "- the training labels\n",
    "\n",
    "- the valiation labels\n",
    "\n",
    "- a **kwargs argument to be passed to the classifier function\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方針\n",
    "\n",
    "1. Training phase\n",
    "    - todo:\n",
    "        - training datasetを使って`classifier(feature_matrix, labels, **kwargs)` functionで(theta, theta_0)をチューニングする\n",
    "    - input:\n",
    "        - `training feature matrix`\n",
    "        - `training labels`\n",
    "    - output: \n",
    "        - `theta`, `theta`, `theta_0`\n",
    "2. Validation phase\n",
    "    - todo:\n",
    "        - 前問で実装した`classify(feature_matrix, theta, theta_0)`を使って、チューニング済み(theta, theta_0)よりラベル付け(+1,-1)を行う\n",
    "    - input1:\n",
    "        - `traing feature matrix`\n",
    "        - `(theta, theta_0)`\n",
    "    - output1;\n",
    "        - traing dataset predictions\n",
    "    - input2:\n",
    "        - `validation feature matrix`\n",
    "        - `(theta, theta_0)`\n",
    "    - output2;\n",
    "        - validation dataset predictions\n",
    "3. Compute the classification accuracy phase\n",
    "    - todo:\n",
    "        - `accuracy(preds, targets)`を使ってtraning/validation predictionsとtraning/validation labelsを比較してclassification accuracyを算出する\n",
    "    - input1:\n",
    "        - training predictions\n",
    "        - training labels\n",
    "    - output1:\n",
    "        - training accuracy\n",
    "    - input2:\n",
    "        - validation predictions\n",
    "        - validation labels\n",
    "    - output2:\n",
    "        - validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_accuracy(\n",
    "        classifier,\n",
    "        train_feature_matrix,\n",
    "        val_feature_matrix,\n",
    "        train_labels,\n",
    "        val_labels,\n",
    "        **kwargs):\n",
    "    \"\"\"\n",
    "    Trains a linear classifier and computes accuracy.\n",
    "    The classifier is trained on the train data. The classifier's\n",
    "    accuracy on the train and validation data is then returned.\n",
    "\n",
    "    Args:\n",
    "        classifier - A classifier function that takes arguments\n",
    "            (feature matrix, labels, **kwargs) and returns (theta, theta_0)\n",
    "        train_feature_matrix - A numpy matrix describing the training\n",
    "            data. Each row represents a single data point.\n",
    "        val_feature_matrix - A numpy matrix describing the training\n",
    "            data. Each row represents a single data point.\n",
    "        train_labels - A numpy array where the kth element of the array\n",
    "            is the correct classification of the kth row of the training\n",
    "            feature matrix.\n",
    "        val_labels - A numpy array where the kth element of the array\n",
    "            is the correct classification of the kth row of the validation\n",
    "            feature matrix.\n",
    "        **kwargs - Additional named arguments to pass to the classifier\n",
    "            (e.g. T or L)\n",
    "\n",
    "    Returns: A tuple in which the first element is the (scalar) accuracy of the\n",
    "    trained classifier on the training data and the second element is the\n",
    "    accuracy of the trained classifier on the validation data.\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    \n",
    "    ## 1. Training phase ##\n",
    "    parameters = classifier(train_feature_matrix, train_labels, **kwargs)\n",
    "    \n",
    "    ## 2. Validation phase\n",
    "    training_predections = classify(train_feature_matrix, parameters[0], parameters[1])\n",
    "    validation_predecions = classify(val_feature_matrix, parameters[0], parameters[1])\n",
    "    \n",
    "    ## 3. Compute the classification accuracy phase\n",
    "    traing_accuracy = p1.accuracy(training_predections, train_labels)\n",
    "    validation_accuracy = p1.accuracy(validation_predecions, val_labels)\n",
    "    \n",
    "    return (traing_accuracy, validation_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### A note on the classifier() call in classifier_accuracy: Python and first-class functions\n",
    "\n",
    "> For folks new to Python or functional programming: In Python, one can store a reference to a function in a variable, and pass that function reference as an argument. Then one can later call the function by using the variable name just as though it were the function name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False,  True])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a reference to an example function and store it in a variable.\n",
    "# This just makes an alias for the function.\n",
    "foo = np.greater\n",
    "# And some arguments for it.\n",
    "a = np.array([1, 2, 3])\n",
    "b = np.array([2, 2, 2])\n",
    "# Now call the function.\n",
    "foo(a, b)\n",
    "# Result is array([False, False,  True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Parameter Tuning\n",
    "\n",
    "> You finally have your algorithms up and running, and a way to measure performance! But, it's still unclear what values the hyperparameters like  𝑇  and  𝜆  should have. In this section, you'll tune these hyperparameters to maximize the performance of each model.\n",
    "\n",
    "---\n",
    "\n",
    "> One way to tune your hyperparameters for any given Machine Learning algorithm is to perform a grid search over all the possible combinations of values. If your hyperparameters can be any real number, you will need to limit the search to some finite set of possible values for each hyperparameter. For efficiency reasons, often you might want to tune one individual parameter, keeping all others constant, and then move onto the next one; Compared to a full grid search there are many fewer possible combinations to check, and this is what you'll be doing for the questions below.\n",
    "\n",
    "> In main.py uncomment Problem 8 to run the staff-provided tuning algorithm from utils.py. For the purposes of this assignment, please try the following values for  𝑇 : [1, 5, 10, 15, 25, 50] and the following values for  𝜆  [0.001, 0.01, 0.1, 1, 10]. For pegasos algorithm, first fix  𝜆=0.01  to tune  𝑇 , and then use the best  𝑇  to tune  𝜆"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword\n",
    "\n",
    "- hyperparameter\n",
    "- grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance After Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result\n",
    "\n",
    "- best method: `Pagasos`\n",
    "\n",
    "```\n",
    "perceptron valid: [(1, 0.758), (5, 0.72), (10, 0.716), (15, 0.778), (25, 0.794), (50, 0.79)]\n",
    "best = 0.7940, T=25.0000\n",
    "avg perceptron valid: [(1, 0.794), (5, 0.792), (10, 0.798), (15, 0.798), (25, 0.8), (50, 0.796)]\n",
    "best = 0.8000, T=25.0000\n",
    "Pegasos valid: tune T [(1, 0.7), (5, 0.782), (10, 0.794), (15, 0.806), (25, 0.804), (50, 0.806)]\n",
    "best = 0.8060, T=15.0000\n",
    "Pegasos valid: tune L [(0.001, 0.79), (0.01, 0.806), (0.1, 0.752), (1, 0.594), (10, 0.518)]\n",
    "best = 0.8060, L=0.0100\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy on the test set\n",
    "\n",
    "> After you have chosen your best method (perceptron, average perceptron or Pegasos) and parameters, use this classifier to compute testing accuracy on the test set. \n",
    "\n",
    "> We have supplied the feature matrix and labels in main.py as test_bow_features and test_labels.\n",
    "\n",
    "> Note: In practice the validation set is used for tuning hyperparameters while a heldout test set is the final benchmark used to compare disparate models that have already been tuned. You may notice that your results using a validation set don't always align with those of the test set, and this is to be expected.\n",
    "\n",
    "\n",
    "```python\n",
    "T=25\n",
    "L=0.0100\n",
    "\n",
    "# Your code here\n",
    "avg_peg_train_accuracy, avg_peg_test_accuracy = \\\n",
    "   p1.classifier_accuracy(p1.pegasos, train_bow_features,test_bow_features,train_labels,test_labels,T=T,L=L)\n",
    "print(\"{:50} {:.4f}\".format(\"Training accuracy for Pegasos:\", avg_peg_train_accuracy))\n",
    "print(\"{:50} {:.4f}\".format(\"Test accuracy for Pegasos:\", avg_peg_test_accuracy))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result\n",
    "\n",
    "```\n",
    "Training accuracy for Pegasos:                     0.9195\n",
    "Test accuracy for Pegasos:                         0.8020\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The most explanatory unigrams\n",
    "\n",
    "> According to the largest weights (i.e. individual  𝑖  values in your vector), you can find out which unigrams were the most impactful ones in predicting positive labels. Uncomment the relevant part in main.py to call utils.most_explanatory_word.\n",
    "\n",
    "> Report the top ten most explanatory word features for positive classification below:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分析\n",
    "\n",
    "- thetaの要素するは`13234`個\n",
    "- typeはnumpyのarray\n",
    "\n",
    "```\n",
    "theta dim: (13234,)\n",
    "theta type: <class 'numpy.ndarray'>\n",
    "```\n",
    "- thetaの中身のトップ10\n",
    "    - thetaを降順にソート\n",
    "\n",
    "```\n",
    "0.5903715521713366\n",
    "\n",
    "0.5685059656607246\n",
    "\n",
    "0.44734183045507536\n",
    "\n",
    "0.40064895432564757\n",
    "\n",
    "0.38411827540030485\n",
    "\n",
    "0.36204152120051414\n",
    "\n",
    "0.3599248573066697\n",
    "\n",
    "0.35073693119923643\n",
    "\n",
    "0.32758790048448905\n",
    "\n",
    "0.3274736683441225\n",
    "```\n",
    "\n",
    "- wordlist[list]のサイズ\n",
    "    - thetaの数と同じ\n",
    "\n",
    "```\n",
    "13234\n",
    "```\n",
    "\n",
    "- `most_explanatory_word(theta, wordlist)`を分析\n",
    "    - thetaの各要素（重み）とwordをzipでひも付けしている\n",
    "    - 降順ソートはこの関数内で行っているので、この関数に渡す前にthetaをソートしてしまうと、wordとのひも付けが間違った状態になってしまう。\n",
    "    - wordlistは13234個あるので、thetaもそのままの要素数で渡す\n",
    "\n",
    "```\n",
    "[word for (theta_i, word) in sorted(zip(theta, wordlist))[::-1]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([2,1,10,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  1,  2, 10])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(a, kind='quicksort')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139985953269360"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[::-1].sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10,  2,  1, -1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139985953269360"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### most_explanatory_word関数内のzipの挙動分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_dummy = np.random.randint(1,10,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 7, 9, 3, 5])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist = ['apple', 'banana', 'grape', 'orange', 'peach']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['apple', 'banana', 'grape', 'orange', 'peach']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### zip関数を使ってみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped = zip(theta_dummy, wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((2, 'apple'), (7, 'banana'), (9, 'grape'), (3, 'orange'), (5, 'peach'))\n"
     ]
    }
   ],
   "source": [
    "print(tuple(zipped))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### zipしたものをソートしてみる\n",
    "\n",
    "Note: [What is the difference between sorted(list) vs list.sort()?](https://stackoverflow.com/questions/22442378/what-is-the-difference-between-sortedlist-vs-list-sort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_zipped = sorted(zip(theta_dummy, wordlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 'apple'), (3, 'orange'), (5, 'peach'), (7, 'banana'), (9, 'grape')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_zipped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### zipしたものを降順でソートしてみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_zipped = sorted(zip(theta_dummy, wordlist))[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9, 'grape'), (7, 'banana'), (5, 'peach'), (3, 'orange'), (2, 'apple')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_zipped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### zipするもの同士の要素数が同じでない場合\n",
    "\n",
    "- #theta < #wordlist\n",
    "- 結果: 少ない要素数に合わせてzipされるので情報落ちが発生する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_dummy2 = np.random.randint(1,10,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_dummy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped2 = zip(theta_dummy2, wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((3, 'apple'), (2, 'banana'))\n"
     ]
    }
   ],
   "source": [
    "print(tuple(zipped2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方針\n",
    "\n",
    "- best_thetaにはtrain dataとpegasosアルゴリズムで出したthetaをそのまま代入\n",
    "    - そのままとは、ソートや絞り込みなどは一切行わないということ\n",
    "- pegasosでdata setを回してparameterを抽出するという関数がないので、`classifier_accuracy` @main.pyを真似て、`classifier_parameter`をmain.pyに作成する\n",
    "- イメージ:\n",
    "    1. text(review) -> bow_features(1,0で構成されるベクトル)に変換。文字を数値化することで、アルゴリズムの計算ができるようにする。\n",
    "    2. bow_featuresのtest datasetでアルゴリズムを用いて、parameter最適化を行う。つまり,最適化されたthetaとtheta_0を求める\n",
    "    3. 求めたthetaはbowのwordlistの要素数と同じ要素数を持つベクトルとなる。この各要素はwordlistの各wordの重みになる。そのため、もう一度このthetaをwordlistと紐付けを行う。(重み:word)というtupleの集合を作成する。そして、重みの降順でのソートも行う。（これが`most_explanatory_word`関数内で行っていること）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 結果\n",
    "\n",
    "- listupされた単語は合っていた。しかし、後半の順位は間違っていたので、やはりpegasosの実装にどこか問題があるみたい\n",
    "\n",
    "```\n",
    "Most Explanatory Word Features\n",
    "['delicious', 'great', '!', 'best', 'perfect', 'loves', 'glad', 'wonderful', 'quickly', 'love']\n",
    "```\n",
    "\n",
    "- ちなみに、昇順ソートにすればworst top10になる\n",
    "\n",
    "```python\n",
    "return [word for (theta_i, word) in sorted(zip(theta, wordlist))]\n",
    "```\n",
    "\n",
    "```\n",
    "['disappointed', 'however', 'bad', 'not', 'but', 'unfortunately', 'awful', '\\$', 'ok', 'money']\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Feature Engineering\n",
    "\n",
    ">Frequently, the way the data is represented can have a significant impact on the performance of a machine learning method. Try to improve the performance of your best classifier by using different features. In this problem, we will practice two simple variants of the bag of words (BoW) representation.\n",
    "\n",
    "---\n",
    "\n",
    "## Remove Stop Words\n",
    "\n",
    ">Try to implement stop words removal in your feature engineering code. Specifically, load the file stopwords.txt, remove the words in the file from your dictionary, and use features constructed from the new dictionary to train your model and make predictions.\n",
    "\n",
    ">Compare your result in the testing data on Pegasos algorithm using  𝑇=25  and  𝐿=0.01  when you remove the words in stopwords.txt from your dictionary.\n",
    "\n",
    ">Hint: Instead of replacing the feature matrix with zero columns on stop words, you can modify the bag_of_words function to prevent adding stopwords to the dictionary\n",
    "\n",
    ">Accuracy on the test set using the original dictionary: 0.8020\n",
    ">Accuracy on the test set using the dictionary with stop words removed:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方針\n",
    "\n",
    "- `stopwords.txt`をloadしてlist化@main.py\n",
    "- `bag_of_words_remove_stop_words(train_texts, stopwords_data)`関数をproject1.pyに作成\n",
    "    - list化したstopwordsを引数で渡せるようにする\n",
    "    - wordを追加する際に、stopwordsを参照して、wordがstopwordsに含まれるのであれば追加しないようにする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [\n",
    "    'i',\n",
    "    'me',\n",
    "    'my',\n",
    "    'myself',\n",
    "    'we',\n",
    "    'our',\n",
    "    'ours',\n",
    "    'ourselves',\n",
    "    'you',\n",
    "    'your',\n",
    "    'yours',\n",
    "    'yourself',\n",
    "    'yourselves',\n",
    "    'he'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = [\n",
    "    'apple',\n",
    "    'my',\n",
    "    'orange'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word apple\n",
      "stop word my\n",
      "word orange\n"
     ]
    }
   ],
   "source": [
    "for w in word_list:\n",
    "    if w not in stopwords:\n",
    "        print('word ' + w)\n",
    "    else:\n",
    "        print('stop word ' + w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 結果\n",
    "\n",
    "- original dictionaryに比べて、dictionary with stop words removedだと確かに精度が上がった\n",
    "- しかし、不正解。やはりpegasosの実装に問題があるよう。\n",
    "\n",
    "```shell\n",
    "Training accuracy for Pegasos:                     0.9150\n",
    "Test accuracy for Pegasos:                         0.8100\n",
    "13108\n",
    "Most Explanatory Word Features\n",
    "['delicious', 'great', 'loves', '!', 'best', 'perfect', 'excellent', 'wonderful', 'favorite', 'love']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_6.86x)",
   "language": "python",
   "name": "conda_6.86x"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
