{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "- install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Downloading https://files.pythonhosted.org/packages/1e/7a/dbb3be0ce9bd5c8b7e3d87328e79063f8b263b2b1bfa4774cb1147bfcd3f/sklearn-0.0.tar.gz\n",
      "Collecting scikit-learn (from sklearn)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/04/49633f490f726da6e454fddc8e938bbb5bfed2001681118d3814c219b723/scikit_learn-0.21.2-cp36-cp36m-manylinux1_x86_64.whl (6.7MB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6.7MB 2.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.17.0 in /home/shouhei/anaconda3/envs/6.86x/lib/python3.6/site-packages (from scikit-learn->sklearn) (1.3.0)\n",
      "Collecting joblib>=0.11 (from scikit-learn->sklearn)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/c1/50a758e8247561e58cb87305b1e90b171b8c767b15b12a1734001f41d356/joblib-0.13.2-py2.py3-none-any.whl (278kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 286kB 23.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.11.0 in /home/shouhei/anaconda3/envs/6.86x/lib/python3.6/site-packages (from scikit-learn->sklearn) (1.16.4)\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/shouhei/.cache/pip/wheels/76/03/bb/589d421d27431bcd2c6da284d5f2286c8e3b2ea3cf1594c074\n",
      "Successfully built sklearn\n",
      "Installing collected packages: joblib, scikit-learn, sklearn\n",
      "Successfully installed joblib-0.13.2 scikit-learn-0.21.2 sklearn-0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn            0.0     \n"
     ]
    }
   ],
   "source": [
    "!pip list | grep sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Linear Regression with Closed Form Solution\n",
    "\n",
    "> After seeing the problem, your classmate Alice immediately argues that we can apply a linear regression model, as the labels are numbers from 0-9, very similar to the example we learned from the lecture. Though being a little doubtful, you decide to have a try and start simple by using the raw pixel values of each image as features.\n",
    "\n",
    "> Alice wrote a skeleton code run_linear_regression_on_MNIST in main.py, but she needs your help to complete the code and make the model work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Closed Form Solution of Linear Regression\n",
    "\n",
    "> To solve the linear regression problem, you recall the linear regression has a closed form solution:\n",
    "\n",
    "$\n",
    "\\displaystyle  \\displaystyle \\theta = (X^ T X + \\lambda I)^{-1} X^ T Y\n",
    "$\n",
    "\n",
    "> where  ùêº  is the identity matrix.\n",
    "\n",
    "> Write a function closed_form that computes this closed form solution given the features  ùëã , labels  ùëå  and the regularization parameter  ùúÜ .\n",
    "\n",
    "- LES„ÅÆÂÖ¨ÂºèÔºàhyper parametar lambda„ÅÆÊ≠£ÂâáÂåñÈ†Ö‰ªò„ÅçÔºâ\n",
    "    - „Çπ„É©„Ç§„Éâ25 https://www.slideshare.net/siritori/6-35685092\n",
    "- closed form = Ëß£ÊûêËß£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ë°åÂàó„ÅÆÁ©ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3,  4],\n",
       "       [ 5,  6,  7,  8],\n",
       "       [ 9, 10, 11, 12]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[1,2,3,4],[5,6,7,8],[9,10,11,12]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 4)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  5,  9],\n",
       "       [ 2,  6, 10],\n",
       "       [ 3,  7, 11],\n",
       "       [ 4,  8, 12]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_t = x.transpose()\n",
    "x_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 3)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tx = np.matmul(x_t, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ÈÄÜË°åÂàó\n",
    "\n",
    "- https://stackoverflow.com/questions/21638895/inverse-of-a-matrix-using-numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.69124961e+13, -5.93736279e+13, -2.19902326e+13,\n",
       "         3.44513643e+13],\n",
       "       [-7.03687442e+13,  7.14682558e+13,  6.81697209e+13,\n",
       "        -6.92692325e+13],\n",
       "       [-0.00000000e+00,  3.51843721e+13, -7.03687442e+13,\n",
       "         3.51843721e+13],\n",
       "       [ 2.34562481e+13, -4.72790000e+13,  2.41892558e+13,\n",
       "        -3.66503876e+11]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_inverse = np.linalg.inv(x_tx)\n",
    "x_inverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Âçò‰ΩçË°åÂàó„ÅÆÁîüÊàê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.identity(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closed_form(X, Y, lambda_factor):\n",
    "    \"\"\"\n",
    "    Computes the closed form solution of linear regression with L2 regularization\n",
    "\n",
    "    Args:\n",
    "        X - (n, d + 1) NumPy array (n datapoints each with d features plus the bias feature in the first dimension)\n",
    "        Y - (n, ) NumPy array containing the labels (a number from 0-9) for each\n",
    "            data point\n",
    "        lambda_factor - the regularization constant (scalar)\n",
    "    Returns:\n",
    "        theta - (d + 1, ) NumPy array containing the weights of linear regression. Note that theta[0]\n",
    "        represents the y-axis intercept of the model and therefore X[0] = 1\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    X_T = X.transpose()\n",
    "    X_TX = np.matmul(X_T, X)\n",
    "    dim_X = X.shape[1]\n",
    "    I = np.identity(dim_X)\n",
    "    theta = np.matmul(np.linalg.inv(X_TX + lambda_factor*I), np.matmul(X_T,Y))\n",
    "    return theta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Error on Linear Regression\n",
    "\n",
    "> Apply the linear regression model on the test set. For classification purpose, you decide to round the predicted label into numbers 0-9.\n",
    "\n",
    "> Note: For this project we will be looking at the error rate defined as the fraction of labels that don't match the target labels, also known as the \"gold labels\" or ground truth. (In other context, you might want to consider other performance measures such as precision and recall, which we have not discussed in this course).\n",
    "\n",
    "- [precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall)\n",
    "\n",
    "> Please enter the test error of your linear regression algorithm for different  ùúÜ  (copy the output from the main.py run)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What went Wrong?\n",
    "\n",
    "> Alice and you find that no matter what  ùúÜ  factor you try, the test error is large. With some thinking, you realize that something is wrong with this approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Support Vector Machine\n",
    "\n",
    ">Bob thinks it is clearly not a regression problem, but a classification problem. He thinks that we can change it into a binary classification and use the support vector machine we learned in Lecture 4 to solve the problem. In order to do so, he suggests that we can build an one vs. rest model for every digit. For example, classifying the digits into two classes: 0 and not 0.\n",
    "\n",
    ">Bob wrote a function run_svm_one_vs_rest_on_MNIST where he changed the labels of digits 1-9 to 1 and keeps the label 0 for digit 0. He also found that sklearn package contains an SVM model that you can use directly. He gave you the link to this model and hopes you can tell him how to use that.\n",
    "\n",
    "- SVC is Support Vector Machine Classifiers\n",
    "    - [sklearn.svm.LinearSVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn-svm-linearsvc)\n",
    "    - [Plot different SVM classifiers in the iris dataset](https://scikit-learn.org/stable/auto_examples/svm/plot_iris_svc.html#plot-different-svm-classifiers-in-the-iris-dataset)\n",
    "    - [1.4. Support Vector Machines](https://scikit-learn.org/stable/modules/svm.html#support-vector-machines)\n",
    "- one-vs-the-rest: [one-vs-the-rest](https://scikit-learn.org/stable/modules/multiclass.html#one-vs-the-rest)\n",
    "- Choosing the right estimator\n",
    "    - classification -> Linear SVC\n",
    "![](./ml_map.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_vs_rest_svm(train_x, train_y, test_x):\n",
    "    \"\"\"\n",
    "    Trains a linear SVM for binary classifciation\n",
    "\n",
    "    Args:\n",
    "        train_x - (n, d) NumPy array (n datapoints each with d features)\n",
    "        train_y - (n, ) NumPy array containing the labels (0 or 1) for each training data point\n",
    "        test_x - (m, d) NumPy array (m datapoints each with d features)\n",
    "    Returns:\n",
    "        pred_test_y - (m,) NumPy array containing the labels (0 or 1) for each test data point\n",
    "    \"\"\"\n",
    "    clf = LinearSVC(random_state = 0, C=0.1)\n",
    "    pred_tes_y = clf.fit(train_x, train_y).predict(test_x)\n",
    "    return pred_tes_y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary classification error\n",
    "\n",
    "> Report the test error by running run_svm_one_vs_rest_on_MNIST.\n",
    "\n",
    "`compute_test_error_svm(test_y, pred_test_y)`@svm.py„ÇíÂÆüË£Ö„Åô„Çã„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = np.array([1,0,1,1])\n",
    "pred_y = np.array([1,1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(pred_y == test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement C-SVM\n",
    "\n",
    "> Play with the C parameter of SVM, what statement is true about the C parameter?\n",
    "\n",
    "- `LinearSVC(random_state = 0, C=0.1)`„ÅÆC„Éë„É©„É°„Éº„Çø„ÅÆ„Åì„Å®\n",
    "    > Penalty parameter C of the error term.\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC.decision_function\n",
    "- https://stats.stackexchange.com/questions/31066/what-is-the-influence-of-c-in-svms-with-linear-kernel\n",
    "\n",
    "- Answer\n",
    "    - Larger C gives smaller tolerance of violation.\n",
    "    - Larger C gives a smaller-margin separating hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Multinomial (Softmax) Regression and Gradient Descent\n",
    "\n",
    "> Daniel suggests that instead of building ten models, we can expand a single logistic regression model into a multinomial regression and solve it with similar gradient descent algorithm.\n",
    "\n",
    "> The main function which you will call to run the code you will implement in this section is run_softmax_on_MNIST in main.py (already implemented). In the appendix at the bottom of this page, we describe a number of the methods that are already implemented for you in softmax.py that will be useful.\n",
    "\n",
    "> In order for the regression to work, you will need to implement three methods. Below we describe what the functions should do. We have included some test cases in test.py to help you verify that the methods you have implemented are behaving sensibly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Probabilities for Softmax\n",
    "\n",
    "> Write a function compute_probabilities that computes, for each data point  ùë•(ùëñ) , the probability that  ùë•(ùëñ)  is labeled as  ùëó  for  ùëó=0,1,‚Ä¶,ùëò‚àí1 .\n",
    "\n",
    "> The softmax function  ‚Ñé  for a particular vector  ùë•  requires computing\n",
    "\n",
    "$\n",
    "h(x) = \\frac{1}{\\sum _{j=1}^ k e^{\\theta _ j \\cdot x / \\tau }} \\begin{bmatrix}  e^{\\theta _1 \\cdot x / \\tau } \\\\ e^{\\theta _2 \\cdot x / \\tau } \\\\ \\vdots \\\\ e^{\\theta _ k \\cdot x / \\tau } \\end{bmatrix}\n",
    "$\n",
    "\n",
    "> where  ùúè>0  is the temperature parameter . When computing the output probabilities (they should always be in the range  [0,1] ), the terms  ùëíùúÉùëó‚ãÖùë•/ùúè  may be very large or very small, due to the use of the exponential function. This can cause numerical or overflow errors. To deal with this, we can simply subtract some fixed amount  ùëê  from each exponent to keep the resulting number from getting too large. Since\n",
    "\n",
    "$\n",
    "h(x) = \\frac{e^{-c}}{e^{-c}\\sum _{j=1}^ k e^{\\theta _ j \\cdot x / \\tau }} \\begin{bmatrix}  e^{\\theta _1 \\cdot x / \\tau } \\\\ e^{\\theta _2 \\cdot x / \\tau } \\\\ \\vdots \\\\ e^{\\theta _ k \\cdot x / \\tau } \\end{bmatrix} \\\\ = \\frac{1}{\\sum _{j=1}^ k e^{[\\theta _ j \\cdot x / \\tau ] - c}} \\begin{bmatrix}  e^{[\\theta _1 \\cdot x / \\tau ] - c} \\\\ e^{[\\theta _2 \\cdot x / \\tau ] - c} \\\\ \\vdots \\\\ e^{[\\theta _ k \\cdot x / \\tau ] - c} \\end{bmatrix}\n",
    "$\n",
    "\n",
    "subtracting some fixed amount  ùëê  from each exponent will not change the final probabilities. A suitable choice for this fixed amount is $c = \\max _ j \\theta _ j \\cdot x / \\tau$ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_probabilities(X, theta, temp_parameter):\n",
    "    \"\"\"\n",
    "    Computes, for each datapoint X[i], the probability that X[i] is labeled as j\n",
    "    for j = 0, 1, ..., k-1\n",
    "\n",
    "    Args:\n",
    "        X - (n, d) NumPy array (n datapoints each with d features)\n",
    "        theta - (k, d) NumPy array, where row j represents the parameters of our model for label j\n",
    "        temp_parameter - the temperature parameter of softmax function (scalar)\n",
    "    Returns:\n",
    "        H - (k, n) NumPy array, where each entry H[j][i] is the probability that X[i] is labeled as j\n",
    "    \"\"\"\n",
    "    #YOUR CODE HERE\n",
    "    k = theta.shape[0]\n",
    "    n = X.shape[0]\n",
    "    H = np.zeros((k,n))\n",
    "    c = np.zeros(n)\n",
    "    print('-----------------------------------')\n",
    "    print(X)\n",
    "    print(theta)\n",
    "    print(temp_parameter)\n",
    "    print(c)\n",
    "            \n",
    "    for i in range(n):\n",
    "        x_i = X[i]\n",
    "        for j in range(k):\n",
    "            theta_j = theta[j]\n",
    "            c[i] = max(c[i], np.dot(theta_j, x_i)/temp_parameter)\n",
    "            H[j][i] = np.dot(theta_j, x_i)/temp_parameter\n",
    "    print(c)\n",
    "    H_T = np.transpose(H)\n",
    "    print(H_T)\n",
    "    for i in range(n):\n",
    "        coefficient = np.exp(H_T[i] - c[i]).sum()\n",
    "        print('cofficient: ' + str(coefficient))\n",
    "        H_T_i = np.exp(H_T[i] - c[i]) / coefficient\n",
    "        print(H_T_i)\n",
    "        H_T[i] = H_T_i\n",
    "        \n",
    "    H = np.transpose(H_T)\n",
    "    print('-------Result-------')\n",
    "    print(H)\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.71828183, 2.71828183, 2.71828183])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-7353191a3a73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/6.86x/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mappend\u001b[0;34m(arr, values, axis)\u001b[0m\n\u001b[1;32m   4692\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4693\u001b[0m         \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4694\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "np.append(H, a, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init = np.ones((3,4))\n",
    "init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init[0][0] = 2\n",
    "init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 1., 1., 1.],\n",
       "       [2., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init[1][0] = 2\n",
    "init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 1., 1., 1.],\n",
       "       [2., 1., 1., 1.],\n",
       "       [2., 1., 1., 1.]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init[2][0] = 2\n",
    "init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4., 4., 4.])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = init.transpose()[0]*2\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4., 4., 4.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_T = init.transpose()\n",
    "init_T[0] = d\n",
    "init_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.0"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_T[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(np.array([1,2,99,4,5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function\n",
    "\n",
    "> Write a function compute_cost_function that computes the total cost over every data point.\n",
    "\n",
    "> The cost function  ùêΩ(ùúÉ)  is given by: (Use natural log)\n",
    "\n",
    "$\n",
    "J(\\theta ) = -\\frac{1}{n}\\Bigg[\\sum _{i=1}^ n \\sum _{j={{0}} }^{{{k-1}} } [[y^{(i)} == j]] \\log {\\frac{e^{\\theta _ j \\cdot x^{(i)} / \\tau }}{\\sum _{l={{0}} }^{{{k-1}} } e^{\\theta _ l \\cdot x^{(i)} / \\tau }}}\\Bigg] + \\frac{\\lambda }{2}\\sum _{{{j=0}} }^{{{k-1}} }\\sum _{{{i}} =0}^{d-1} \\theta _{{{ji}} }^2\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_function(X, Y, theta, lambda_factor, temp_parameter):\n",
    "    \"\"\"\n",
    "    Computes the total cost over every datapoint.\n",
    "\n",
    "    Args:\n",
    "        X - (n, d) NumPy array (n datapoints each with d features)\n",
    "        Y - (n, ) NumPy array containing the labels (a number from 0-9) for each\n",
    "            data point\n",
    "        theta - (k, d) NumPy array, where row j represents the parameters of our\n",
    "                model for label j\n",
    "        lambda_factor - the regularization constant (scalar)\n",
    "        temp_parameter - the temperature parameter of softmax function (scalar)\n",
    "\n",
    "    Returns\n",
    "        c - the cost value (scalar)\n",
    "    \"\"\"\n",
    "    n, k, d = X.shape[0], theta.shape[0], theta.shape[1]\n",
    "    H = compute_probabilities(X, theta, temp_parameter)\n",
    "    first_term_cost = 0.0\n",
    "    second_term_cost = 0.0\n",
    "    H_T = H.transpose()\n",
    "    \n",
    "    # compute first term of cost function\n",
    "    for i in range(n):\n",
    "        x_i = H_T[i]\n",
    "        for j in range(k):\n",
    "            if Y[i] != j:\n",
    "                first_term_cost += 0\n",
    "            else:\n",
    "                first_term_cost += np.log(H_T[i][j])\n",
    "                \n",
    "    # compute second term of cost function\n",
    "    for i in range(k):\n",
    "        for j in range(d):\n",
    "            second_term_cost += theta[i][j]**2\n",
    "    \n",
    "    c = - (first_term_cost / n) + (lambda_factor * second_term_cost / 2)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_term = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_term += 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "> Write a function run_gradient_descent_iteration that runs one step of batch gradient descent.\n",
    "\n",
    "> The partial derivative of  ùêΩ(ùúÉ)  wrt a particular  ùúÉùëó  is given by:\n",
    "\n",
    "$\n",
    "\\nabla _{\\Theta _ j} J(\\theta ) = -\\frac{1}{\\tau n} \\sum _{i = 1} ^{n} [x^{(i)}([[y^{(i)} == j]] - p(y^{(i)} = j | x^{(i)}, \\theta ))] + \\lambda \\theta _ j\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution to this problem available before due date: \n",
    "\n",
    "> The function run_gradient_descent_iteration is necessary for the rest of the project. Hence, once you have either submitted the correct function or finished your attempts for this problem, the solution to this function will be available. We are sorry we acted this late, but hope that it will still help.\n",
    "\n",
    "> Now, in order to run the gradient descent algorithm to minimize the cost function, we need to take the derivative of  ùêΩ(ùúÉ)  wrt a particular  ùúÉùëö . Notice that within  ùêΩ(ùúÉ) , we have:\n",
    "\n",
    "$\n",
    "\\frac{e^{\\theta _ j \\cdot x^{(i)} / \\tau }}{\\sum _{l=0}^{k-1} e^{\\theta _ l \\cdot x^{(i)} / \\tau }} = p(y^{(i)} = j | x^{(i)}, \\theta )\n",
    "$\n",
    "\n",
    "so we first compute: $\\frac{\\partial p(y^{(i)} = j | x^{(i)}, \\theta )}{\\partial \\theta _ m}$\n",
    "\n",
    "when $m=j$,\n",
    "\n",
    "$\n",
    "\\frac{\\partial p(y^{(i)} = j | x^{(i)}, \\theta )}{\\partial \\theta _ m}=\\frac{x^{(i)}}{\\tau }p(y^{(i)} = m | x^{(i)}, \\theta )[1-p(y^{(i)} = m | x^{(i)}, \\theta )]\n",
    "$\n",
    "\n",
    "when $m\\neq j$\n",
    "\n",
    "$\n",
    "\\frac{\\partial p(y^{(i)} = j | x^{(i)}, \\theta )}{\\partial \\theta _ m}=- \\frac{x^{(i)}}{\\tau }p(y^{(i)} = m | x^{(i)}, \\theta )p(y^{(i)} = j | x^{(i)}, \\theta )\n",
    "$\n",
    "\n",
    "Now we compute\n",
    "\n",
    "$\n",
    "\\displaystyle  \\displaystyle \\frac{\\partial }{\\partial \\theta _ m} \\Bigg[ \\sum _{j=0}^{k-1} [[y^{(i)} == j]] \\log {\\frac{e^{\\theta _ j \\cdot x^{(i)} / \\tau }}{\\sum _{l=0}^{k-1} e^{\\theta _ l \\cdot x^{(i)} / \\tau }}} \\Bigg]\n",
    "\\displaystyle  = \\sum _{j=0, j\\neq m}^{k-1} \\Bigg[ [[y^{(i)} == j]] [- \\frac{x^{(i)}}{\\tau }p(y^{(i)} = m | x^{(i)}, \\theta )] \\Bigg]\n",
    "\\displaystyle + [[y^{(i)} == m]] \\frac{x^{(i)}}{\\tau }[1-p(y^{(i)} = m | x^{(i)}, \\theta )]\n",
    "$\n",
    "$\n",
    "\\displaystyle  = \\frac{x^{(i)}}{\\tau } \\Bigg[ [[y^{(i)} == m]] - p(y^{(i)} = m | x^{(i)}, \\theta ) \\sum _{j=0}^{k-1} [[y^{(i)} == j]] \\Bigg]\n",
    "$\n",
    "\n",
    "$\n",
    "\\displaystyle  = \\frac{x^{(i)}}{\\tau } \\Bigg[ [[y^{(i)} == m]] - p(y^{(i)} = m | x^{(i)}, \\theta ) \\Bigg]\n",
    "$\n",
    "\n",
    "Plug this into the derivatite of  $J(\\theta )$ , we have\n",
    "\n",
    "$\n",
    "\\displaystyle  \\displaystyle \\frac{\\partial J(\\theta )}{\\partial \\theta _ m}\n",
    "$\n",
    "$\n",
    "\\displaystyle  = \\frac{\\partial }{\\partial \\theta _ m}\\Bigg[-\\frac{1}{n}\\Bigg[\\sum _{i=1}^ n \\sum _{j=0}^{k-1} [[y^{(i)} == j]] \\log p(y^{(i)} = j | x^{(i)}, \\theta ) \\Bigg] + \\frac{\\lambda }{2}\\sum _{j=0}^{k-1}\\sum _{i=0}^{d-1} \\theta _{ji}^2\\Bigg]\n",
    "$\n",
    "\n",
    "$\n",
    "\\displaystyle  = -\\frac{1}{\\tau n} \\sum _{i = 1} ^{n} [x^{(i)}([[y^{(i)} == m]] - p(y^{(i)} = m | x^{(i)}, \\theta ))] + \\lambda \\theta _ m\n",
    "$\n",
    "\n",
    "To run gradient descent, we will update $\\theta$ at each step with $\\theta \\leftarrow \\theta - \\alpha \\nabla _{\\theta } J(\\theta )$, where $\\alpha$ is the learning rate.\n",
    "\n",
    "Write a function `run_gradient_descent_iteration` that runs one step of the gradient descent algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ToDo: ‰ª•‰∏ã„ÅÆÁêÜÁî±„ÅßÊîπÂñÑÁÇπ„Åå„ÅÑ„Åè„Å§„Åã„ÅÇ„Çã\n",
    "    - `sparse.coo_martix`‰ΩøÁî®„Åó„Å¶„ÅÑ„Å™„ÅÑ\n",
    "    - MNIST„ÅÆ„Éá„Éº„Çø„Åß„ÅÆÂÆüË°åÊôÇÈñì„Åå40ÂàÜ„Åª„Å©„Åã„Åã„Çã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gradient_descent_iteration(X, Y, theta, alpha, lambda_factor, temp_parameter):\n",
    "    \"\"\"\n",
    "    Runs one step of batch gradient descent\n",
    "\n",
    "    Args:\n",
    "        X - (n, d) NumPy array (n datapoints each with d features)\n",
    "        Y - (n, ) NumPy array containing the labels (a number from 0-9) for each\n",
    "            data point\n",
    "        theta - (k, d) NumPy array, where row j represents the parameters of our\n",
    "                model for label j\n",
    "        alpha - the learning rate (scalar)\n",
    "        lambda_factor - the regularization constant (scalar)\n",
    "        temp_parameter - the temperature parameter of softmax function (scalar)\n",
    "\n",
    "    Returns:\n",
    "        theta - (k, d) NumPy array that is the final value of parameters theta\n",
    "    \"\"\"\n",
    "    #YOUR CODE HERE\n",
    "    n, k, d = X.shape[0], theta.shape[0], theta.shape[1]\n",
    "    H = compute_probabilities(X, theta, temp_parameter)\n",
    "    H_T = H.transpose()\n",
    "    \n",
    "    for m in range(k):\n",
    "        first_term_grad_cost = 0.0\n",
    "        second_term_grad_cost = 0.0\n",
    "        for i in range(n):\n",
    "            if Y[i] != m:\n",
    "                first_term_grad_cost += X[i]*(-H_T[i][m])\n",
    "            else:\n",
    "                first_term_grad_cost += X[i]*(1-H_T[i][m])\n",
    "        first_term_grad_cost  = first_term_grad_cost / (- temp_parameter * n)\n",
    "        second_term_grad_cost = lambda_factor * theta[m]\n",
    "        grad_cost = first_term_grad_cost + second_term_grad_cost\n",
    "        for l in range(d):\n",
    "            theta[m][l] -= alpha * grad_cost[l] \n",
    "        \n",
    "    return theta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scipy.sparse.coo_matrix\n",
    "\n",
    "- sparse matrix: ÁñéË°åÂàóÔºà„Çπ„Éë„Éº„ÇπË°åÂàóÔºâ„ÄÇÊàêÂàÜ„ÅÆ„Åª„Å®„Çì„Å©„Åå„Çº„É≠„Åß„ÅÇ„ÇãË°åÂàó\n",
    "- ‰ªäÂõû„ÅØ`COO(Â∫ßÊ®ô„É™„Çπ„Éà)`„Åß„ÅÆÊ†ºÁ¥çÊñπÊ≥ï„Çí‰Ωø„ÅÜ\n",
    "\n",
    "> You should use sparse.coo_martix so that your function can handle larger matrices efficiently (and not time out for the online graders). The sparse matrix representation can handle sparse matrices efficiently.\n",
    "\n",
    "- [scipy.sparse.coo_matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.coo_matrix.html)\n",
    "- [ÁñéË°åÂàó](https://ja.wikipedia.org/wiki/%E7%96%8E%E8%A1%8C%E5%88%97)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Constructing an empty matrix\n",
    "coo_matrix((3,4)).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Â∫ßÊ®ô„ÇíÊåáÂÆö„Åó„Å¶„Å™„ÅÑË¶ÅÁ¥†„ÅØ0„Å´„Å™„Çã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 0, 9, 0],\n",
       "       [0, 7, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 5]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Constructing a matrix using ijv format\n",
    "row  = np.array([0, 3, 1, 0])\n",
    "col  = np.array([0, 3, 1, 2])\n",
    "data = np.array([4, 5, 7, 9])\n",
    "coo_matrix((data, (row, col)), shape=(4,4)).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Âêå‰∏ÄÂ∫ßÊ®ô„ÇíË§áÊï∞ÊåáÂÆö„Åô„Çã„Å®„Åù„ÅÆdata„ÅåË∂≥„Åï„Çå„Çã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Constructing a matrix with duplicate indices\n",
    "row  = np.array([0, 0, 1, 3, 1, 0, 0])\n",
    "col  = np.array([0, 2, 1, 3, 1, 0, 0])\n",
    "data = np.array([1, 1, 1, 1, 1, 1, 1])\n",
    "coo = coo_matrix((data, (row, col)), shape=(4, 4))\n",
    "# Duplicate indices are maintained until implicitly or explicitly summed\n",
    "np.max(coo.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 0, 1, 0],\n",
       "       [0, 2, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 1]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coo.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = np.array([1,0,1])\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = Y.shape[0]\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This is how to use scipy's sparse.coo_matrix function to create a sparse matrix of 0's and 1's:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0],\n",
       "       [1, 0, 1],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0]])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = coo_matrix(([1]*n, (Y, range(n))), shape=(k,n)).toarray()\n",
    "M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This will create a normal numpy array with 1s and 0s.\n",
    "\n",
    "> On larger inputs (i.e. MNIST), this is  10x faster than using a naive for loop. (See example code if interested).\n",
    "\n",
    "> Note: As a personal challenge, try to see if you can use special numpy functions to add 1 in-place. This would be even faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.585542917251587\n",
      "0.3834571838378906\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "ITER = 100\n",
    "K = 10\n",
    "N = 10000\n",
    "\n",
    "def naive(indices, k):\n",
    "    mat = [[1 if i == j else 0 for j in range(k)] for i in indices]\n",
    "    return np.array(mat).T\n",
    "\n",
    "\n",
    "def with_sparse(indices, k):\n",
    "    n = len(indices)\n",
    "    M = sparse.coo_matrix(([1]*n, (Y, range(n))), shape=(k,n)).toarray()\n",
    "    return M\n",
    "\n",
    "\n",
    "Y = np.random.randint(0, K, size=N)\n",
    "\n",
    "t0 = time.time()\n",
    "for i in range(ITER):\n",
    "    naive(Y, K)\n",
    "print(time.time() - t0)\n",
    "\n",
    "\n",
    "t0 = time.time()\n",
    "for i in range(ITER):\n",
    "    with_sparse(Y, K)\n",
    "print(time.time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## numpy.concatenate\n",
    "- row„ÇíÈÄ£Áµê„Åó„Å¶np.array„ÇíÁîüÊàê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-135-67a32771197d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrow1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mrow2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrow2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "row1 = np.array([1,2,3])\n",
    "row2 = np.array([4,5,6])\n",
    "np.concatenate((row1,row2), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Temperature\n",
    "\n",
    "> We will now explore the effects of the temperature parameter in our algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting Error Rates\n",
    "\n",
    "- $\\text {Error}|_{T=0.5} =$ 0.08399999999999996\n",
    "\n",
    "    ![temp-para=0.5](temp-para=0.5.png)\n",
    "\n",
    "- $\\text {Error}|_{T=1} =$ 0.10050000000000003\n",
    "\n",
    "    ![temp-para=1](temp-para=1.png)\n",
    "\n",
    "- $\\text {Error}|_{T=2} =$ 0.1261\n",
    "\n",
    "    ![temp-para=2](temp-para=2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Changing Labels\n",
    "\n",
    "> We now wish to classify the digits by their (mod 3) value, such that the new label  ùë¶(ùëñ)  of sample  ùëñ  is the old  ùë¶(ùëñ)(mod3) . (Reminder: Return the temp_parameter to be 1 if you changed it for the last section)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Current Model - update target\n",
    "\n",
    ">Given that we already classified every  ùë•(ùëñ)  as a digit, we could use the model we already trained and just calculate our estimations (mod 3).\n",
    "\n",
    ">Implement update_y function, which changes the old digit labels for the training and test set for the new (mod 3) labels.\n",
    "\n",
    "- digit: 0-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_y(train_y, test_y):\n",
    "    \"\"\"\n",
    "    Changes the old digit labels for the training and test set for the new (mod 3)\n",
    "    labels.\n",
    "\n",
    "    Args:\n",
    "        train_y - (n, ) NumPy array containing the labels (a number between 0-9)\n",
    "                 for each datapoint in the training set\n",
    "        test_y - (n, ) NumPy array containing the labels (a number between 0-9)\n",
    "                for each datapoint in the test set\n",
    "\n",
    "    Returns:\n",
    "        train_y_mod3 - (n, ) NumPy array containing the new labels (a number between 0-2)\n",
    "                     for each datapoint in the training set\n",
    "        test_y_mod3 - (n, ) NumPy array containing the new labels (a number between 0-2)\n",
    "                    for each datapoint in the test set\n",
    "    \"\"\"\n",
    "    #YOUR CODE HERE\n",
    "    return train_y % 3, test_y %3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digit = np.array([range(10)])\n",
    "digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2, 0, 1, 2, 0, 1, 2, 0]])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digit % 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 0, 1, 2, 0, 1, 2, 0])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y = np.array([0,1,2,3,4,5,6,7,8,9])\n",
    "test_y = np.array([0,1,2,3,4,5,6,7,8,9])\n",
    "update_y(train_y, test_y)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Current Model - compute test error\n",
    "\n",
    "> Implement compute_test_error_mod3 function, which takes the test points X, their correct labels Y (digits (mod 3) from 0-2), theta, and the temp_parameter, and returns the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_test_error_mod3(X, Y, theta, temp_parameter):\n",
    "    \"\"\"\n",
    "    Returns the error of these new labels when the classifier predicts the digit. (mod 3)\n",
    "\n",
    "    Args:\n",
    "        X - (n, d - 1) NumPy array (n datapoints each with d - 1 features)\n",
    "        Y - (n, ) NumPy array containing the labels (a number from 0-2) for each\n",
    "            data point\n",
    "        theta - (k, d) NumPy array, where row j represents the parameters of our\n",
    "                model for label j\n",
    "        temp_parameter - the temperature parameter of softmax function (scalar)\n",
    "\n",
    "    Returns:\n",
    "        test_error - the error rate of the classifier (scalar)\n",
    "    \"\"\"\n",
    "    #YOUR CODE HERE\n",
    "    estimated_Y = get_classification(X, theta, temp_parameter)\n",
    "    estimated_Y_mod3 = estimated_Y % 3\n",
    "    \n",
    "    test_error = 1 - np.mean(Y == estimated_Y_mod3)\n",
    "    \n",
    "    return test_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Using the Current Model - test error\n",
    "\n",
    "> Find the error rate of the new labels (call these two functions at the end of run_softmax_on_MNIST). See the functions' documentation for detailed explanations of the inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_MNIST_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-144-44f86bc992bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'softmax test_error='\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_softmax_on_MNIST\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_parameter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'softmax test_error_mod3='\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_softmax_on_MNIST\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_parameter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-144-44f86bc992bf>\u001b[0m in \u001b[0;36mrun_softmax_on_MNIST\u001b[0;34m(temp_parameter)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mFinal\u001b[0m \u001b[0mtest\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \"\"\"\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_MNIST_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost_function_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_parameter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mplot_cost_function_over_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost_function_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_MNIST_data' is not defined"
     ]
    }
   ],
   "source": [
    "def run_softmax_on_MNIST(temp_parameter=1):\n",
    "    \"\"\"\n",
    "    Trains softmax, classifies test data, computes test error, and plots cost function\n",
    "\n",
    "    Runs softmax_regression on the MNIST training set and computes the test error using\n",
    "    the test set. It uses the following values for parameters:\n",
    "    alpha = 0.3\n",
    "    lambda = 1e-4\n",
    "    num_iterations = 150\n",
    "\n",
    "    Saves the final theta to ./theta.pkl.gz\n",
    "\n",
    "    Returns:\n",
    "        Final test error\n",
    "    \"\"\"\n",
    "    train_x, train_y, test_x, test_y = get_MNIST_data()\n",
    "    theta, cost_function_history = softmax_regression(train_x, train_y, temp_parameter, alpha=0.3, lambda_factor=1.0e-4, k=10, num_iterations=150)\n",
    "    plot_cost_function_over_time(cost_function_history)\n",
    "    test_error = compute_test_error(test_x, test_y, theta, temp_parameter)\n",
    "    # Save the model parameters theta obtained from calling softmax_regression to disk.\n",
    "    write_pickle_data(theta, \"./theta.pkl.gz\")\n",
    "\n",
    "    # TODO: add your code here for the \"Using the Current Model\" question in tab 4.\n",
    "    #      and print the test_error_mod3\n",
    "    test_y_mod3 = update_y(train_y, test_y)[1]\n",
    "    test_error_mod3 = compute_test_error_mod3(test_x, test_y_mod3, theta, temp_parameter)\n",
    "    return test_error, test_error_mod3\n",
    "\n",
    "\n",
    "print('softmax test_error=', run_softmax_on_MNIST(temp_parameter=1)[0])\n",
    "print('softmax test_error_mod3=', run_softmax_on_MNIST(temp_parameter=1)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain with New Labels\n",
    "\n",
    "> Now suppose that instead we want to retrain our classifier with the new labels. In other words, rather than training the model to predict the original digits and then taking those predictions modulo 3, we explicitly train the model to predict the digits modulo 3 from the original image.\n",
    "\n",
    "> How do you expect the performance to change using the new labels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_softmax_on_MNIST_mod3(temp_parameter=1):\n",
    "    \"\"\"\n",
    "    Trains Softmax regression on digit (mod 3) classifications.\n",
    "\n",
    "    See run_softmax_on_MNIST for more info.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    train_x, train_y, test_x, test_y = get_MNIST_data()\n",
    "    train_y_mod3, test_y_mod3 = update_y(train_y, test_y)[0], update_y(train_y, test_y)[1]\n",
    "    theta, cost_function_history = softmax_regression(train_x, train_y_mod3, temp_parameter, alpha=0.3, lambda_factor=1.0e-4, k=10, num_iterations=150)\n",
    "    plot_cost_function_over_time(cost_function_history)\n",
    "    test_error_mod3 = compute_test_error_mod3(test_x, test_y_mod3, theta, temp_parameter)\n",
    "    # Save the model parameters theta obtained from calling softmax_regression to disk.\n",
    "    write_pickle_data(theta, \"./theta.pkl.gz\")\n",
    "\n",
    "    # TODO: add your code here for the \"Using the Current Model\" question in tab 4.\n",
    "    #      and print the test_error_mod3\n",
    "    return test_error_mod3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax(Multinomial) Regression model„ÅÆ„Åæ„Å®„ÇÅ\n",
    "\n",
    "- 4. Multinomial (Softmax) Regression\n",
    "- 6. Changing Labels -> label„Çímod3„Å´Â§âÊõ¥\n",
    "- 8. Dimensionality Reduction Using PCA -> PCA„ÇíÁî®„ÅÑ„Å¶„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆdimension„ÇíËêΩ„Å®„Åó„Å¶„ÄÅSoftmax„ÇíÂÆüË°å\n",
    "- 9. Cubic Features -> cubuic feature„ÇíÈÅ©Áî®„Åô„Çã„ÄÇ„Åü„Å†„Åó„ÄÅraw dataset„ÅÆ„Åæ„ÅæÈÅ©Áî®„Åô„Çã„Å®È´òÊ¨°ÂÖÉ„Å´„Å™„Å£„Å¶„Åó„Åæ„ÅÜ„Åü„ÇÅ„ÄÅPCA„ÅßÊ¨°ÂÖÉ„ÇíËêΩ„Å®„Åó„Å¶„Åã„ÇâÈÅ©Áî®„Åô„Çã\n",
    "\n",
    "|No.| Function                      | Traing data set         | Test error„ÇíÁÆóÂá∫„Å´‰Ωø„Å£„Åüdata set                |Test error| Description                                          |\n",
    "|:---:|:---------------------------:|:-----------------------:| :-------------------------------------------:|:----:|:----------------------------------------------------:|\n",
    "|4,6| run_softmax_on_MNIST          | (train_x, train_y)      | (test_x, test_y), (test_x, test_y_mod3)      |0.10050000000000003, 0.07679999999999998(mod3)| lable„Åå0-9(digit)„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åßtraining„Åó„Å¶theata„ÇíÊ±Ç„ÇÅ„Çã  |\n",
    "|6| run_softmax_on_MNIST_mod3       | (train_x, train_y_mod3) | (test_x, test_y_mod3)                        |0.18720000000000003| lable„Åå0,1,2(mod3)„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åßtraining„Åó„Å¶theata„ÇíÊ±Ç„ÇÅ„Çã |\n",
    "|8| run_softmax_on_MNIST_pca        | (train_pca, train_y)    | (test_pca, test_y)                           |0.1483             | 784-dim„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„Çípca„Å´„Çà„Çä18-dim„Å´„Åó„Å¶training„Åó„Å¶theata„ÇíÊ±Ç„ÇÅ„Çã |\n",
    "|9|run_softmax_on_MNIST_pca10_cubic |(train_cube, train_y)    | (test_cube, test_y)                          |0.08640000000000003| 784-dim„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„Çípcs„Å´„Çà„Çä10-dim„Å´„Åó„Å¶„ÄÅ„Åï„Çâ„Å´cubic kernel„Å´ÈÅ©Áî®„Åó„Åü„Éá„Éº„Çø„Éª„Çª„ÉÉ„Éà„Åßtraining„Åó„Å¶theta„ÇíÊ±Ç„ÇÅ„Çã|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# 7. Classification Using Manually Crafted Features\n",
    "\n",
    "> The performance of most learning algorithms depends heavily on the representation of the training data. In this section, we will try representing each image using different features in place of the raw pixel values. Subsequently, we will investigate how well our regression model from the previous section performs when fed different representations of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction via PCA\n",
    "\n",
    "> Principal Components Analysis (PCA) is the most popular method for linear dimension reduction of data and is widely used in data analysis. For an in-depth exposition see: \n",
    "[http://snobear.colorado.edu/Markw/BioMath/Otis/PCA/principal_components.ps](http://snobear.colorado.edu/Markw/BioMath/Otis/PCA/principal_components.ps)\n",
    "\n",
    ">Briefly, this method finds (orthogonal) directions of maximal variation in the data. By projecting an $n \\times d$ dataset  $X$ onto  $k \\leq d$  of these directions, we get a new dataset of lower dimension that reflects more variation in the original data than any other  $k$-dimensional linear projection of $ùëã$ . By going through some linear algebra, it can be proven that these directions are equal to the  $ùëò$ eigenvectors corresponding to the $ùëò$ largest eigenvalues of the covariance matrix  $\\widetilde{X}^ T \\widetilde{X}$ , where  $\\widetilde{X}$  is a centered version of our original data.\n",
    "\n",
    "\n",
    "> Remark: The best implementations of PCA actually use the Singular Value Decomposition of $\\widetilde{X}$ rather than the more straightforward approach outlined here, but these concepts are beyond the scope of this course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cubic Features\n",
    "\n",
    "In this section, we will also work with a cubic feature mapping which maps an input vector  $x = [x_1,\\dots , x_ d]$  into a new feature vector  $\\phi (x)$ , defined so that for any  $x, x' \\in \\mathbb {R}^ d$:\n",
    "\n",
    "$\n",
    "\\phi (x)^ T \\phi (x') = (x^ T x' + 1)^3\n",
    "$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([-0.4035,  0.915])\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.array([0.915 ,  0.4035])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.05194688, 0.51665608, 0.54277497, 0.2346091 , 0.12200716],\n",
       "       [0.12782774, 0.48055288, 0.62581373, 0.56599878, 0.36839801],\n",
       "       [0.57735819, 0.60481533, 0.70493835, 0.63290267, 0.10632069],\n",
       "       [0.34825322, 0.32315491, 0.82751388, 0.60932118, 0.61791211],\n",
       "       [0.32421745, 0.19315761, 0.15519407, 0.04214597, 0.81223093]])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = np.random.rand(5,5)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05194688, 0.12782774, 0.57735819, 0.34825322, 0.32421745])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.51665608, 0.48055288, 0.60481533, 0.32315491, 0.19315761])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.05194688, 0.51665608, 0.54277497],\n",
       "       [0.12782774, 0.48055288, 0.62581373],\n",
       "       [0.57735819, 0.60481533, 0.70493835],\n",
       "       [0.34825322, 0.32315491, 0.82751388],\n",
       "       [0.32421745, 0.19315761, 0.15519407]])"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[:,range(3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Calculate the eigenvectors and eignevalues of the covariance matrix\n",
    "\n",
    "- pricipal_components.ps„ÅÆÂÜÖÂÆπ„ÇíÁ¢∫Ë™ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.61655556, 0.61544444],\n",
       "       [0.61544444, 0.71655556]])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov = np.array([[.616555556, .615444444],[.615444444, .716555556]])\n",
    "cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [numpy.linalg.eig](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.linalg.eig.html)\n",
    "    - column„Ååeigen vector\n",
    "    \n",
    "    > The normalized (unit ‚Äúlength‚Äù) eigenvectors, such that the column v[:,i] is the eigenvector corresponding to the eigenvalue w[i].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.0490834 , 1.28402771]), array([[-0.73517866, -0.6778734 ],\n",
       "        [ 0.6778734 , -0.73517866]]))"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.eig(cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.0490834 , 1.28402771])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('eigenvalues:')\n",
    "np.linalg.eig(cov)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvectors, eigenvalue:\n",
      "u1 :[-0.73517866  0.6778734 ] eigenvalue :0.0490833998257566\n",
      "u2 :[-0.6778734  -0.73517866] eigenvalue :1.2840277121742432\n"
     ]
    }
   ],
   "source": [
    "v1 =  np.linalg.eig(cov)[0][0]\n",
    "v2 =  np.linalg.eig(cov)[0][1]\n",
    "u1 = np.linalg.eig(cov)[1][:,0]\n",
    "u2 = np.linalg.eig(cov)[1][:,1]\n",
    "\n",
    "print('eigenvectors, eigenvalue:')\n",
    "print('u1 :' + str(u1) + ' eigenvalue :' + str(v1))\n",
    "print('u2 :' + str(u2) + ' eigenvalue :' + str(v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u1 norm :0.9999999999999999\n",
      "u2 norm :0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "print('u1 norm :' + str(np.linalg.norm(u1)))\n",
    "print('u2 norm :' + str(np.linalg.norm(u2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u1@u2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "O = np.array([0,0])\n",
    "\n",
    "plt.quiver(\n",
    "    O[0],O[1],\n",
    "    u1[0],u1[1],\n",
    "    angles='xy', scale_units='xy', scale=1\n",
    ")\n",
    "\n",
    "plt.quiver(\n",
    "    O[0],O[1],\n",
    "    u2[0],u2[1],\n",
    "    angles='xy', scale_units='xy', scale=1\n",
    ")\n",
    "\n",
    "\n",
    "plt.xlim([-2,2])\n",
    "plt.ylim([-2,2])\n",
    "plt.grid()\n",
    "plt.draw()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## principle component„ÅÆÂà§Êñ≠„ÅÆ‰ªïÊñπ P.16\n",
    "\n",
    "> the eigenvector with the highest eigenvalue is the principle component of the data set.\n",
    "\n",
    "„Å§„Åæ„Çä‰ªäÂõû„Å†„Å®„ÄÅÊ¨°„ÅÆeigenvector„Ååprinciple component„Å®„Å™„Çã(eigenvalue„Åå‰∏ÄÁï™Â§ß„Åç„ÅÑeigenvector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u2 :[-0.6778734  -0.73517866] eigenvalue :1.2840277121742432\n"
     ]
    }
   ],
   "source": [
    "print('u2 :' + str(u2) + ' eigenvalue :' + str(v2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA„ÅÆÊµÅ„Çå\n",
    "\n",
    "1. covariance matrix„ÇíÊ±Ç„ÇÅ„Çã\n",
    "1. covariance matrix„ÅÆeigenvector(eigenvalue)„ÇíÊ±Ç„ÇÅ„ÇãÔºà„Åì„Åì„Åæ„Åß„Çí‰∏äË®ò„ÅßË°å„Å£„ÅüÔºâ\n",
    "1. eigenvectors„ÇíË¶ã„Å§„Åë„Åü„Çâ„ÄÅ„Åù„ÅÆeigenvector„Çíeigenvalue„ÅßÈôçÈ†Ü„Åß„ÇΩ„Éº„Éà„Åô„Çã\n",
    "    - „Åì„Çå„Åå„Åì„ÅÆdata set„ÅßÈáçË¶Å„Å™ÊñπÂêëÈ†Ü„Å®„Å™„Çã„ÄÇ\n",
    "    - ‰∏ä„Åß„ÇÇËø∞„Åπ„Åü„Åå„ÄÅeigenvalue„Åå‰∏ÄÁï™Â§ß„Åç„ÅÑeigenvector„Åå‰∏ÄÁï™ÈáçË¶Å(„Å§„Åæ„Çäprinciple component)\n",
    "    - „Å°„Å™„Åø„Å´„ÄÅeigenvector = component(Ë¶ÅÁ¥†)„Å®Âëº„Çì„Åß„ÅÑ„Çã\n",
    "    > This give you the components in order of significance.\n",
    "1. „Åù„ÅÆ‰∏≠„Åã„ÇâÂ•Ω„Åç„Å™„Çà„ÅÜ„Å´„ÅÇ„Åæ„ÇäÈáçË¶Å„Åß„Å™„ÅÑcomponents„ÇíÈô§„ÅÑ„Å¶„ÅÑ„Åè\n",
    "    > Now, if you like, you can decide to ignore the components of lesser siginicance. You do lose some information, but if the eigenvalues are small, you don't lose much.\n",
    "    \n",
    "    > To be precise, if you originally have n dimensions in your data, and so you calculate n eigenvectors and eigenvalues, and the you choose only the first p eigenvectors, then the final data set has only p dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## „Éó„É≠„Ç∞„É©„Éü„É≥„Ç∞„ÅÆ„Åü„ÇÅ„ÅÆÁ∑öÂΩ¢‰ª£Êï∞ P. xi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1. , -0.3],\n",
       "       [-0.7,  0.6]])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[1,-0.3],[-0.7,0.6]])\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvectors, eigenvalue:\n",
      "u1 :[ 0.70710678 -0.70710678] eigenvalue :1.2999999999999998\n",
      "u2 :[0.3939193  0.91914503] eigenvalue :0.3\n"
     ]
    }
   ],
   "source": [
    "av1 = np.linalg.eig(A)[0][0]\n",
    "av2 = np.linalg.eig(A)[0][1]\n",
    "au1 = np.linalg.eig(A)[1][:,0]\n",
    "au2 = np.linalg.eig(A)[1][:,1]\n",
    "\n",
    "print('eigenvectors, eigenvalue:')\n",
    "print('u1 :' + str(au1) + ' eigenvalue :' + str(av1))\n",
    "print('u2 :' + str(au2) + ' eigenvalue :' + str(av2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "\n",
    "O = np.array([0,0])\n",
    "\n",
    "plt.quiver(\n",
    "    O[0],O[1],\n",
    "    au1[0],au1[1],\n",
    "    angles='xy', scale_units='xy', scale=1\n",
    ")\n",
    "\n",
    "plt.quiver(\n",
    "    O[0],O[1],\n",
    "    au2[0],au2[1],\n",
    "    angles='xy', scale_units='xy', scale=1\n",
    ")\n",
    "\n",
    "\n",
    "plt.xlim([-2,2])\n",
    "plt.ylim([-2,2])\n",
    "plt.grid()\n",
    "plt.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Dimensionality Reduction Using PCA\n",
    "\n",
    "> PCA finds (orthogonal) directions of maximal variation in the data. In this problem we're going to project our data onto the principal components and explore the effects on performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project onto Principal Components\n",
    "\n",
    "> Fill in function project_onto_PC in features.py that implements PCA dimensionality reduction of dataset $X$.\n",
    "\n",
    "> $\\widetilde{X} V$\n",
    "\n",
    "> where $\\widetilde{X}$ is the centered version of the original data $X$ and $V$ is the $d \\times k$ matrix whose columns are the top $k$ eigenvectors of $\\widetilde{X}^ T \\widetilde{X}$. This is because the eigenvectors are of unit-norm, so there is no need to divide by their length.\n",
    "\n",
    "> You are given the full principal component matrix $V'$ as pcs in this function.\n",
    "\n",
    "> Available Functions: You have access to the NumPy python library as np and the function center_data which returns a centered version of the data, where each feature now has mean = 0\n",
    "\n",
    "- $X$ : ÂÖÉ„ÅÆdataset\n",
    "- $\\widetilde{X}$ : centered„Åó„Åüdataset [`X_center_ajusted`]\n",
    "- $V'$ : covariance matrix„ÅÆeigen vector„ÅÆÈõÜ„Åæ„Çä„ÄÇfull principal component matrix„ÄÄ[`pcs`]\n",
    "- $V$ : $V'$„Åã„Çâtop k components„Å†„Åë„ÇíÊäΩÂá∫„Åó„Åümatrix [`feature_vector`]\n",
    "- $\\widetilde{X} V$ : projcet $X$ into its k-dim PCA representaion. $X$„Çí$n$-dim„Åã„Çâ$k$-dim„Å∏„ÅÆÂ∞ÑÂΩ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_onto_PC(X, pcs, n_components):\n",
    "    \"\"\"\n",
    "    Given principal component vectors pcs = principal_components(X)\n",
    "    this function returns a new data array in which each sample in X\n",
    "    has been projected onto the first n_components principcal components.\n",
    "    \"\"\"\n",
    "    # TODO: first center data using the centerData() function.\n",
    "    # TODO: Return the projection of the centered dataset\n",
    "    #       on the first n_components principal components.\n",
    "    #       This should be an array with dimensions: n x n_components.\n",
    "    # Hint: these principal components = first n_components columns\n",
    "    #       of the eigenvectors returned by principal_components().\n",
    "    #       Note that each eigenvector is already be a unit-vector,\n",
    "    #       so the projection may be done using matrix multiplication.\n",
    "    X_center_ajusted = center_data(X)\n",
    "    feature_vector = pcs[:,range(n_components)]\n",
    "    final_data_T = feature_vector.transpose()@X_center_ajusted.transpose()\n",
    "    return final_data_T.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: we only use the training dataset to determine the principal components. It is improper to use the test dataset for anything except evaluating the accuracy of our predictive model. If the test data is used for other purposes such as selecting good features, it is possible to overfit the test set and obtain overconfident estimates of a model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing PCA\n",
    "\n",
    "> Use project_onto_PC to compute a 18-dimensional PCA representation of the MNIST training and test datasets, as illustrated in main.py.\n",
    "\n",
    "> Retrain your softmax regression model (using the original labels) on the MNIST training dataset and report its error on the test data, this time using these 18-dimensional PCA-representations rather than the raw pixel values.\n",
    "\n",
    "> If your PCA implementation is correct, the model should perform nearly as well when only given 18 numbers encoding each image as compared to the 784 in the original data (error on the test set using PCA features should be around 0.15). This is because PCA ensures these 18 feature values capture the maximal amount of variation from the original 784-dimensional data.\n",
    "\n",
    "- pca„Åßd=784 -> n=18„Å´Ê¨°ÂÖÉ„ÇíËêΩ„Å®„Åô\n",
    "\n",
    "> Use plot_PC in main.py to visualize the first 100 MNIST images, as represented in the space spanned by the first 2 principal components of the training data. What does your PCA look like?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Dimensionality_Reduction_Using_PCA](./Dimensionality_Reduction_Using_PCA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Remark: Two dimensional PCA plots offer a nice way to visualize some global structure in high-dimensional data, although approaches based on nonlinear dimension reduction may be more insightful in certain cases. Notice that for our data, the first 2 principal components are insufficent for fully separating the different classes of MNIST digits.\n",
    "\n",
    "- top 2„ÅÆprincipal components„Åß„Éá„Éº„Çø„ÇíÂèØË¶ñÂåñ„ÄÇ„Åó„Åã„Åó„ÄÅÂõ≥„Åã„Çâ„Çè„Åã„Çã„Çà„ÅÜ„Å´2 components„Å†„Åë„Åß„ÅØclssify„ÅØ„Åß„Åç„Å™„ÅÑ„Åì„Å®„Åå„Çè„Åã„Çã"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Cubic Features\n",
    "\n",
    "In this section, we will work with a cubic feature mapping which maps an input vector $x = [x_1,\\dots , x_ d]$ into a new feature vector $\\phi (x)$, defined so that for any $x, x' \\in \\mathbb {R}^ d$:\n",
    "\n",
    "$\\phi (x)^ T \\phi (x') = (x^ T x' + 1)^3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Cubic Features\n",
    "\n",
    "In 2-D, let $x = [x_1, x_2]$. Write down the explicit cubic feature mapping $\\phi (x)$ as a vector; i.e., $\\phi (x) = [f_1(x_1, x_2), \\cdots , f_ N(x_1, x_2)]$\n",
    "\n",
    "$\\phi (x) = [(x_1)^3,(x_2)^3,sqrt(3)*(x_1)^2*(x_2),sqrt(3)*(x_1)^2,sqrt(3)*(x_2)^2,sqrt(3)*(x_1)*(x_2)^2,sqrt(6)*(x_1)*(x_2),sqrt(3)*(x_1),sqrt(3)*(x_2),1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The cubic_features function in features.py is already implemented for you. That function can handle input with an arbitrary dimension and compute the corresponding features for the cubic Kernel. Note that here we don't leverage the kernel properties that allow us to do a more efficient computation with the kernel function (without computing the features themselves). Instead, here we do compute the cubic features explicitly and apply the PCA on the output features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying to MNIST\n",
    "\n",
    "\n",
    "> If we explicitly apply the cubic feature mapping to the original 784-dimensional raw pixel features, the resulting representation would be of massive dimensionality. Instead, we will apply the quadratic feature mapping to the 10-dimensional PCA representation of our training data which we will have to calculate just as we calculated the 18-dimensional representation in the previous problem. After applying the cubic feature mapping to the PCA representations for both the train and test datasets, retrain the softmax regression model using these new features and report the resulting test set error below.\n",
    "\n",
    "- PCA„Åß784-dim -> 10-dim„Å´ËêΩ„Å®„Åô\n",
    "- 10-dim„ÅÆdataset„Çícubic kernel„Å´‰ª£ÂÖ•„Åô„Çã\n",
    "- „Åù„ÅÆcubic kernel„ÅÆdataset„Åßsoftmax regression model„Åßtraining„ÅóÁõ¥„Åô„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'principal_components' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-234-afaff2a7f00a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mn_components\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpcs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprincipal_components\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtrain_pca10\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproject_onto_PC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpcs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtest_pca10\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproject_onto_PC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpcs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'principal_components' is not defined"
     ]
    }
   ],
   "source": [
    "## Cubic Kernel ##\n",
    "# TODO: Find the 10-dimensional PCA representation of the training and test set\n",
    "\n",
    "n_components = 10\n",
    "pcs = principal_components(train_x)\n",
    "train_pca10 = project_onto_PC(train_x, pcs, n_components)\n",
    "test_pca10 = project_onto_PC(test_x, pcs, n_components)\n",
    "\n",
    "# TODO: First fill out cubicFeatures() function in features.py as the below code requires it.\n",
    "\n",
    "train_cube = cubic_features(train_pca10)\n",
    "test_cube = cubic_features(test_pca10)\n",
    "\n",
    "def run_softmax_on_MNIST_pca10_cubic(temp_parameter=1):\n",
    "    \"\"\"\n",
    "    Trains softmax, classifies test data, computes test error, and plots cost function\n",
    "\n",
    "    Runs softmax_regression on the MNIST training set and computes the test error using\n",
    "    the test set. It uses the following values for parameters:\n",
    "    alpha = 0.3\n",
    "    lambda = 1e-4\n",
    "    num_iterations = 150\n",
    "\n",
    "    Saves the final theta to ./theta.pkl.gz\n",
    "\n",
    "    Returns:\n",
    "        Final test error\n",
    "    \"\"\"\n",
    "    train_x, train_y, test_x, test_y = get_MNIST_data()\n",
    "    theta, cost_function_history = softmax_regression(train_cube, train_y, temp_parameter, alpha=0.3, lambda_factor=1.0e-4, k=10, num_iterations=150)\n",
    "    plot_cost_function_over_time(cost_function_history)\n",
    "    test_error_pca10_cube = compute_test_error(test_cube, test_y, theta, temp_parameter)\n",
    "    # Save the model parameters theta obtained from calling softmax_regression to disk.\n",
    "    write_pickle_data(theta, \"./theta.pkl.gz\")\n",
    "\n",
    "    return test_error_pca10_cube"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_6.86x)",
   "language": "python",
   "name": "conda_6.86x"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
