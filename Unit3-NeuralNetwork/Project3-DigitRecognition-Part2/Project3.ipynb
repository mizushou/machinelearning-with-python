{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project3 Digit recoginition(Part2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Good programmers can use neural nets. Great programmers can make them. This section will guide you through the implementation of a simple neural net with an architecture as shown in the figure below. You will implement the net from scratch (you will probably never do this again, don't worry) so that you later feel confident about using libraries. We provide some skeleton code in neural_nets.py for you to fill in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![d](./images/neural-nets.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Activation Functions\n",
    "\n",
    "> The first step is to design the activation function for each neuron. In this problem, we will initialize the network weights to 1, use ReLU for the activation function of the hidden layers, and use an identity function for the output neuron. The hidden layer has a bias but the output layer does not. Complete the helper functions in neural_networks.py, including rectified_linear_unit and rectified_linear_unit_derivative, for you to use in the NeuralNetwork class, and implement them below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まず、上図のモデルを実装するためにactivation functionを用意する。\n",
    "\n",
    "- For Hidden layer\n",
    "    - ReLU function\n",
    "        - `rectified_linear_unit(x)` 以下で実装\n",
    "    - Backpropagationのために、１階微分の値を返す関数も用意する\n",
    "        - `rectified_linear_unit_derivative(x)` 以下で実装\n",
    "    > use ReLU for the activation function of the hidden layers\n",
    "- For Output layer\n",
    "    - identify function\n",
    "        - `output_layer_activation(x)` neural_nets.pyに実装済み\n",
    "    - Backpropagationのために、１階微分の値を返す関数も用意する\n",
    "        - `output_layer_activation_derivative(x)` neural_nets.pyに実装済み\n",
    "    >  use an identity function for the output neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rectified Linear Unit\n",
    "\n",
    "> First implement the ReLu activation function, which computes the ReLu of a scalar.\n",
    "\n",
    "> **Note**: Your function does not need to handle a vectorized input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def rectified_linear_unit(x):\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rectified_linear_unit_derivative(x):\n",
    "    \"\"\" Returns the derivative of ReLU.\"\"\"\n",
    "    return 1 if x > 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Forward propagation is simply the summation of the previous layer's output multiplied by the weight of each wire, while back-propagation works by computing the partial derivatives of the cost function with respect to every weight or bias in the network. In back propagation, the network gets better at minimizing the error and predicting the output of the data being used for training by incrementally updating their weights and biases using stochastic gradient descent.\n",
    "\n",
    "> We are trying to estimate a continuous-valued function, thus we will use squared loss as our cost function and an identity function as the output activation function. $f(x)$ is the activation function that is called on the input to our final layer output node, and $\\hat{a}$ is the predicted value, while $y$ is the actual value of the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $C = \\frac{1}{2}(y - \\hat{a})^{2}$    (5.1)\n",
    "\n",
    "> $f(x) = x$     (5.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> When you're done implementing the function train (below and in your local repository), run the script and see if the errors are decreasing. If your errors are all under 0.15 after the last training iteration then you have implemented the neural network training correctly.\n",
    "\n",
    "> You'll notice that the train functin inherits from NeuralNetworkBase in the codebox below; this is done for grading purposes. In your local code, you implement the function directly in your Neural Network class all in one file. The rest of the code in NeuralNetworkBase is the same as in the original NeuralNetwork class you have locally.\n",
    "\n",
    "> **In this problem, you will see the network weights are initialized to 1. This is a bad setting in practice, but we do so for simplicity and grading here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Train\n",
    "\n",
    "> Note: Functions rectified_linear_unit_derivative, and output_layer_activation_derivative can only handle scalar input. You will need to use np.vectorize to use them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hidden_layer_errorについて\n",
    "\n",
    "> I just ignored these variables and calculated the gradients and I got correct answer. They are probably given to store intermediate calculations to help calculate gradients, but I thought they were confusing and just skipped ahead. Hope that helps :)\n",
    "\n",
    "> First, it is just a local variable and nobody is actually checking the correctness of the variable if you implement gradients computation right. \n",
    "Second, personally I would consider it as $\\frac{\\partial Loss}{\\partial Output\\ of\\ the\\ hidden\\ layer}$ , since that's how people organize a giant deep neural network into blocks - Each layer is a block, during the forward pass, the output of each block is computed and feed to the next layer as input, and similarly during the backward pass, the gradient of latter block is passed to the previous block as input. So, here it is a good way to use a variable to store the gradient of \"hidden-output\" layer. You will find that you can use the variable to compute the gradient of all the previous layers.\n",
    "\n",
    "> the hidden layer error is delta_1*x. For which how to derive delta_1 is in lecture 9 problem, 2. Back-propagation Algorithm. Expand the answer last problem of this page. For this problem you will be needing a delta_1 for hidden layer error and delta_2 for hidden to output layer error. Then u multiply these errors with another parameter to get your gradient which you use for updating the weights. ps. bias is equal to hidden layer error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-10-788be51bb8b6>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-788be51bb8b6>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    hidden_layer_weighted_input = # TODO (3 by 1 matrix)\u001b[0m\n\u001b[0m                                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(NeuralNetworkBase):\n",
    "\n",
    "    def train(self, x1, x2, y):\n",
    "\n",
    "        ### Forward propagation ###\n",
    "        input_values = np.matrix([[x1],[x2]]) # 2 by 1\n",
    "\n",
    "        # Calculate the input and activation of the hidden layer\n",
    "        hidden_layer_weighted_input = # TODO (3 by 1 matrix)\n",
    "        hidden_layer_activation = # TODO (3 by 1 matrix)\n",
    "\n",
    "        output =  # TODO\n",
    "        activated_output = # TODO\n",
    "\n",
    "        ### Backpropagation ###\n",
    "\n",
    "        # Compute gradients\n",
    "        output_layer_error = # TODO\n",
    "        hidden_layer_error = # TODO (3 by 1 matrix)\n",
    "\n",
    "        bias_gradients = # TODO\n",
    "        hidden_to_output_weight_gradients = # TODO\n",
    "        input_to_hidden_weight_gradients = # TODO\n",
    "\n",
    "        # Use gradients to adjust weights and biases using gradient descent\n",
    "        self.biases = # TODO\n",
    "        self.input_to_hidden_weights = # TODO\n",
    "        self.hidden_to_output_weights = # TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def train(self, x1, x2, y):\n",
    "\n",
    "        ### Forward propagation ###\n",
    "        input_values = np.matrix([[x1],[x2]]) # 2 by 1\n",
    "\n",
    "        # Calculate the input and activation of the hidden layer\n",
    "        hidden_layer_weighted_input = (self.input_to_hidden_weights * input_values) + self.biases# TODO (3 by 1 matrix)　[z1, z2, z3]\n",
    "        v_rectified_linear_unit = np.vectorize(rectified_linear_unit)\n",
    "        hidden_layer_activation = v_rectified_linear_unit(hidden_layer_weighted_input)# TODO (3 by 1 matrix) [f1(z1), f1(z2), f1(z3)]\n",
    "\n",
    "        output = np.dot(self.hidden_to_output_weights, hidden_layer_activation) # TODO [u1]        \n",
    "        activated_output = output_layer_activation(output) # TODO [f2(u1)]\n",
    "\n",
    "        ### Backpropagation ###\n",
    "\n",
    "        # Compute gradients\n",
    "        output_layer_error = (output_layer_activation(output) - y) * output_layer_activation_derivative(output) # TODO\n",
    "        v_rectified_linear_unit_derivative = np.vectorize(rectified_linear_unit_derivative)\n",
    "        hidden_layer_error = np.multiply(self.hidden_to_output_weights.transpose(), v_rectified_linear_unit_derivative(hidden_layer_weighted_input)) * output_layer_error # TODO (3 by 1 matrix)\n",
    "        bias_gradients = hidden_layer_error # TODO\n",
    "        hidden_to_output_weight_gradients = np.multiply(hidden_layer_activation, output_layer_error) # TODO (3 by 1 matrix)\n",
    "        input_to_hidden_weight_gradients =  hidden_layer_error * input_values.transpose() # TODO (3 by 2 matrix)\n",
    "\n",
    "        # Use gradients to adjust weights and biases using gradient descent\n",
    "        self.biases = self.biases - self.learning_rate * bias_gradients # TODO\n",
    "        self.input_to_hidden_weights = self.input_to_hidden_weights - self.learning_rate * input_to_hidden_weight_gradients # TODO\n",
    "        self.hidden_to_output_weights = self.hidden_to_output_weights - self.learning_rate * hidden_to_output_weight_gradients.transpose() # TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用意されているスケルトンコードでnumpy.matrixを使っているので少し慣れる必要がある\n",
    "\n",
    "[numpy.matrix](https://docs.scipy.org/doc/numpy/reference/generated/numpy.matrix.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = 1\n",
    "x2 = 2\n",
    "input_values = np.matrix([[x1],[x2]])\n",
    "input_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_to_hidden_weights = np.matrix('1 2; 3 4; 5 6')\n",
    "input_to_hidden_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]]\n",
      "[[3 4]]\n",
      "[[5 6]]\n"
     ]
    }
   ],
   "source": [
    "for i in input_to_hidden_weights:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 1, 1]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_to_output_weights = np.matrix('1 1 1')\n",
    "hidden_to_output_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[3],\n",
       "        [2],\n",
       "        [1]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases = np.matrix('3; 2; 1')\n",
    "biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1],\n",
       "        [2],\n",
       "        [3]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases2 = np.matrix('1; 2; 3')\n",
    "biases2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[4],\n",
       "        [4],\n",
       "        [4]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases + biases2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_to_hidden_weights.item(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 4],\n",
       "       [5, 6]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert from matrix to ndarray\n",
    "x = input_to_hidden_weights.getA()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 5],\n",
       "        [11],\n",
       "        [17]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# z1,z2,z3を計算. (3 by 1)\n",
    "input_to_hidden_weights*input_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numpy.vectorizeでscalarだけでなく、numpy arraysなどを引数として受けれるようにする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "vrectified_linear_unit_derivative = np.vectorize(rectified_linear_unit_derivative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vrectified_linear_unit_derivative(input_to_hidden_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1],\n",
       "        [1],\n",
       "        [1]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vrectified_linear_unit_derivative(biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases = np.matrix('1; 1; 1')\n",
    "biases.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (3,1) and (3,1) not aligned: 1 (dim 1) != 3 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-74f71300196b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbiases\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbiases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/6.86x/lib/python3.6/site-packages/numpy/matrixlib/defmatrix.py\u001b[0m in \u001b[0;36m__mul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;31m# This promotes 1-D vectors to row vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__rmul__'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (3,1) and (3,1) not aligned: 1 (dim 1) != 3 (dim 0)"
     ]
    }
   ],
   "source": [
    "biases * biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1],\n",
       "        [1],\n",
       "        [1]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.multiply(biases, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_to_output_weights = np.matrix('1 1 1')\n",
    "hidden_to_output_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[3]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(hidden_to_output_weights, biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Predicting the Test Data\n",
    "\n",
    ">Now fill in the code for the function predict, which will use your trained neural network in order to label new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, x1, x2):\n",
    "\n",
    "    input_values = np.matrix([[x1],[x2]])\n",
    "\n",
    "    # Compute output for a single input(should be same as the forward propagation in training)\n",
    "    hidden_layer_weighted_input = (self.input_to_hidden_weights * input_values) + self.biases # TODO\n",
    "\n",
    "    v_rectified_linear_unit = np.vectorize(rectified_linear_unit)\n",
    "\n",
    "    hidden_layer_activation = v_rectified_linear_unit(hidden_layer_weighted_input) # TODO\n",
    "    output = np.dot(self.hidden_to_output_weights, hidden_layer_activation) # TODO\n",
    "    activated_output = activated_output = output_layer_activation(output) # TODO\n",
    "\n",
    "    return activated_output.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conceptual Questions\n",
    "\n",
    ">Now let's review some concepts involved in training Neural Nets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thinking about Network Size\n",
    "\n",
    ">What is the danger to having too many hidden units in your network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It will take up more memory\n",
    "- It may overfit the training data\n",
    "- It will take longer to train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing Accuracy Over Time\n",
    "\n",
    ">What would happen if you run the code for more epochs in terms of training and testing accuracy? Which of the following should we expect to see?\n",
    "\n",
    "`Ecpoch`はトレーニングデータセットを完全に通すこと"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![accuracy-epoch](./images/accuracy-epoch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Classification for MNIST using deep neural networks\n",
    "\n",
    ">In this section, we are going to use deep neural networks to perform the same classification task as in previous sections. We will use PyTorch, a python deep learning framework. Using a framework like PyTorch means you don't have to implement all of the details (like in the earlier problem) and can spend more time thinking through your high level architecture.\n",
    "\n",
    ">Setup Overview To setup PyTorch, navigate to their website in your browser, select your preferences and begin downloading. Your selection for OS and Package Manager will depend on your local setup. For example, if you are on a Mac and use pip as your Python package manager, select \"OSX\" and \"Pip\". We recommend you select Python version 3 for use with PyTorch. Finally, you are not required to train large models for this course, so you can safely select \"None\" for CUDA. If you have access to a NVIDIA GPU enabled device with the CUDA library installed, and want to try training your neural models on GPUs, feel free to install PyTorch with CUDA selected but you will have to troubleshoot on your own.\n",
    "\n",
    ">Test your installation Once you have successfully installed PyTorch using the instructions on their website, you should test your installation to ensure it is running properly before trying to complete the project. For basic functionality, you can start a python REPL environment with the python command in your terminal. Then try importing PyTorch with import torch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- condaでpytorchをupdateしたら、逆に古くなってエリアス？もついてないのでimportすらできなくなった。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch                   1.0.1           cuda100py36he554f03_0  \n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%conda list | grep pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- そのため、一度condaでtorchをremoveして再度インストール\n",
    "    ```\n",
    "    conda install pytorch-cpu torchvision-cpu -c pytorch\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch-cpu               1.1.0               py3.6_cpu_0    pytorch\n",
      "torchvision-cpu           0.3.0             py36_cuNone_1    pytorch\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%conda list | grep pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- importでエラーが出ないことを確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Fully-Connected Neural Networks\n",
    "\n",
    ">First, we will employ the most basic form of a deep neural network, in which the neurons in adjacent layers are fully connected to one another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing Accuracy Over Time\n",
    "\n",
    ">We have provided a toy example nnet_fc.py in which we have implemented for you a simple neural network. This network has one hidden layer of 10 neurons with a rectified linear unit (ReLU) nonlinearity, as well as an output layer of 10 neurons (one for each digit class). Finally, a softmax function normalizes the activations of the output neurons so that they specify a probability distribution. Reference the PyTorch Documentation and read through it in order to gain a better understanding of the code. Then, try running the code on your computer with the command python3 nnet_fc.py. This will train the network with 10 epochs, where an epoch is a complete pass through the training dataset. Total training time of your network should take no more than a couple of minutes. At the end of training, your model should have an accuracy of more than  %85  on test data.\n",
    "\n",
    ">Note: We are not using a softmax layer because it is already present in the loss: PyTorch's nn.CrossEntropyLoss combines nn.LogSoftMax with nn.NLLLoss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用意された`nnet_fc.py`をそのまま実行する。モデルのトレーニングがされて最終的に精度が表示される。\n",
    "    - 精度: 0.9204727564102564"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nnet_fc.py`のモデル (most basic form of deep neural network, in whic the neurons in adjacent layers are fully connected to one another)\n",
    "    - hidden layer: 1\n",
    "        - neurons: 10\n",
    "        - activation: recitified linear unit(ReLU)\n",
    "    - output: 1\n",
    "        - neurons: 10\n",
    "        - activation: ?\n",
    "    - outputのactivationをsoftmax functionでnormalizesして、確率分布に変換する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下が、`nnet_fc.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-19b0154e1f27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"..\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtrain_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbatchify_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "#! /usr/bin/env python\n",
    "\n",
    "import _pickle as cPickle, gzip\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import utils\n",
    "from utils import *\n",
    "from train_utils import batchify_data, run_epoch, train_model\n",
    "\n",
    "def main():\n",
    "    # Load the dataset\n",
    "    num_classes = 10\n",
    "    X_train, y_train, X_test, y_test = get_MNIST_data()\n",
    "\n",
    "    # Split into train and dev\n",
    "    dev_split_index = int(9 * len(X_train) / 10)\n",
    "    X_dev = X_train[dev_split_index:]\n",
    "    y_dev = y_train[dev_split_index:]\n",
    "    X_train = X_train[:dev_split_index]\n",
    "    y_train = y_train[:dev_split_index]\n",
    "\n",
    "    permutation = np.array([i for i in range(len(X_train))])\n",
    "    np.random.shuffle(permutation)\n",
    "    X_train = [X_train[i] for i in permutation]\n",
    "    y_train = [y_train[i] for i in permutation]\n",
    "\n",
    "    # Split dataset into batches\n",
    "    batch_size = 32\n",
    "    train_batches = batchify_data(X_train, y_train, batch_size)\n",
    "    dev_batches = batchify_data(X_dev, y_dev, batch_size)\n",
    "    test_batches = batchify_data(X_test, y_test, batch_size)\n",
    "\n",
    "    #################################\n",
    "    ## Model specification TODO\n",
    "    model = nn.Sequential(\n",
    "              nn.Linear(784, 10),\n",
    "              nn.ReLU(),\n",
    "              nn.Linear(10, 10),\n",
    "            )\n",
    "    lr=0.1\n",
    "    momentum=0\n",
    "    ##################################\n",
    "\n",
    "    train_model(train_batches, dev_batches, model, lr=lr, momentum=momentum)\n",
    "\n",
    "    ## Evaluate the model on test data\n",
    "    loss, accuracy = run_epoch(test_batches, model.eval(), None)\n",
    "\n",
    "    print (\"Loss on test set:\"  + str(loss) + \" Accuracy on test set: \" + str(accuracy))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Specify seed for deterministic behavior, then shuffle. Do not change seed for official submissions to edx\n",
    "    np.random.seed(12321)  # for reproducibility\n",
    "    torch.manual_seed(12321)  # for reproducibility\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving Accuracy\n",
    "\n",
    ">We would like to try to improve the performance of the model by performing a mini grid search over hyper parameters (note that a full grid search should include more values and combinations). To this end, we will use our baseline model (batch size 32, hidden size 10, learning rate 0.1, momentum 0 and the ReLU activation function) and modify one parameter each time while keeping all others to the baseline. We will use the validation accuracy of the model after training for 10 epochs. For the LeakyReLU activation function, use the default parameters from pyTorch (negative_slope=0.01).\n",
    "\n",
    ">Note: If you run the model multiple times from the same script, make sure to initialize the numpy and pytorch random seeds to 12321 before each run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hyper parameterのチューニングのため、以下のようにmini grid searchを行う"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| hidden size        | batch size           | learning rate  | momentum | activation function for hidden layer | validation accuracy of best performing model| note           |\n",
    "|:------------------:|:--------------------:|:--------------:|:--------:|:------------------------------------:|:-------------------:|:--------------:|\n",
    "| 10                 | 32                   | 0.1            | 0        | ReLU                                 | 0.932487  | baseline model |\n",
    "| 10                 | 64                   | 0.1            | 0        | ReLU                                 | 0.94086  | best perfomance model |\n",
    "| 10                 | 32                   | 0.01            | 0        | ReLU                                 | 0.934659  |  |\n",
    "| 10                 | 32                   | 0.1            | 0.9        | ReLU                                 | 0.901404  |  |\n",
    "| 10                 | 32                   | 0.1            | 0        | LeakyReLU                                 | 0.933322  |  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.870758, 0.912289, 0.91892 , 0.923088, 0.92646 , 0.928442,\n",
       "        0.930702, 0.932276, 0.933888, 0.935147],\n",
       "       [0.92363 , 0.929479, 0.932152, 0.930983, 0.931317, 0.93115 ,\n",
       "        0.930648, 0.932487, 0.932487, 0.932487]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "baseline model\n",
    "\"\"\"\n",
    "\n",
    "base = np.array([\n",
    "    [.870758, .912289, .918920, .923088, .926460, .928442, .930702, .932276, .933888, .935147],\n",
    "    [.923630, .929479, .932152, .930983, .931317, .931150, .930648, .932487, .932487, .932487],\n",
    "])\n",
    "base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== base ======\n",
      "Best Train accuracy: 0.935147\n",
      "Best Val accuracy: 0.932487\n"
     ]
    }
   ],
   "source": [
    "print('====== base ======')\n",
    "print('Best Train accuracy: ' + str(max(base[0])))\n",
    "print('Best Val accuracy: ' + str(max(base[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.851016, 0.910569, 0.917927, 0.922505, 0.924692, 0.927213,\n",
       "        0.929956, 0.931587, 0.933478, 0.934405],\n",
       "       [0.922715, 0.927923, 0.930948, 0.935316, 0.936996, 0.936996,\n",
       "        0.937836, 0.939348, 0.94086 , 0.939852]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "hyper parameter: batch size 32 -> 64\n",
    "\"\"\"\n",
    "\n",
    "batch64 = np.array([\n",
    "    [.851016, .910569, .917927, .922505, .924692, .927213, .929956, .931587, .933478, .934405],\n",
    "    [.922715, .927923, .930948, .935316, .936996, .936996, .937836, .939348, .940860, .939852],\n",
    "])\n",
    "batch64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== batch64 ======\n",
      "Best Train accuracy: 0.934405\n",
      "Best Val accuracy: 0.94086\n"
     ]
    }
   ],
   "source": [
    "print('====== batch64 ======')\n",
    "print('Best Train accuracy: ' + str(max(batch64[0])))\n",
    "print('Best Val accuracy: ' + str(max(batch64[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.720417, 0.882576, 0.897044, 0.904157, 0.90788 , 0.910862,\n",
       "        0.913289, 0.915142, 0.916938, 0.918179],\n",
       "       [0.895388, 0.91494 , 0.922293, 0.926303, 0.92764 , 0.928476,\n",
       "        0.930983, 0.931818, 0.933322, 0.934659]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "hyper parameter: learning rate 0.1 -> 0.01\n",
    "\"\"\"\n",
    "\n",
    "lr = np.array([\n",
    "    [0.720417, 0.882576, 0.897044, 0.904157, 0.907880, 0.910862, 0.913289, 0.915142, 0.916938, 0.918179],\n",
    "    [0.895388, 0.914940, 0.922293, 0.926303, 0.927640, 0.928476, 0.930983, 0.931818, 0.933322, 0.934659],\n",
    "])\n",
    "lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== learning rate ======\n",
      "Best Train accuracy: 0.918179\n",
      "Best Val accuracy: 0.934659\n"
     ]
    }
   ],
   "source": [
    "print('====== learning rate ======')\n",
    "print('Best Train accuracy: ' + str(max(lr[0])))\n",
    "print('Best Val accuracy: ' + str(max(lr[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.823614, 0.855679, 0.868794, 0.882817, 0.88102 , 0.880891,\n",
       "        0.874778, 0.883836, 0.884447, 0.880891],\n",
       "       [0.846925, 0.882186, 0.887868, 0.901404, 0.898563, 0.89756 ,\n",
       "        0.890541, 0.899064, 0.870321, 0.888202]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "hyper parameter: momentum 0 -> 0.9\n",
    "\"\"\"\n",
    "\n",
    "momentum = np.array([\n",
    "    [0.823614, 0.855679, 0.868794, 0.882817, 0.881020, 0.880891, 0.874778, 0.883836, 0.884447, 0.880891],\n",
    "    [0.846925, 0.882186, 0.887868, 0.901404, 0.898563, 0.897560, 0.890541, 0.899064, 0.870321, 0.888202],\n",
    "])\n",
    "momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== momentum ======\n",
      "Best Train accuracy: 0.884447\n",
      "Best Val accuracy: 0.901404\n"
     ]
    }
   ],
   "source": [
    "print('====== momentum ======')\n",
    "print('Best Train accuracy: ' + str(max(momentum[0])))\n",
    "print('Best Val accuracy: ' + str(max(momentum[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.871128, 0.913011, 0.918809, 0.923551, 0.926052, 0.928534,\n",
       "        0.930553, 0.932758, 0.93461 , 0.935147],\n",
       "       [0.922627, 0.92881 , 0.929646, 0.930481, 0.930816, 0.930816,\n",
       "        0.931317, 0.933322, 0.932487, 0.931985]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "hyper parameter: ReLU -> LeakyReLU \n",
    "\"\"\"\n",
    "\n",
    "LeakyReLU = np.array([\n",
    "    [0.871128, 0.913011, 0.918809, 0.923551, 0.926052, 0.928534, 0.930553, 0.932758, 0.934610, 0.935147],\n",
    "    [0.922627, 0.928810, 0.929646, 0.930481, 0.930816, 0.930816, 0.931317, 0.933322, 0.932487, 0.931985],\n",
    "])\n",
    "LeakyReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[LeakyReLU function](https://pytorch.org/docs/stable/nn.html#leakyrelu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== LeakyReLU ======\n",
      "Best Train accuracy: 0.935147\n",
      "Best Val accuracy: 0.933322\n"
     ]
    }
   ],
   "source": [
    "print('====== LeakyReLU ======')\n",
    "print('Best Train accuracy: ' + str(max(LeakyReLU[0])))\n",
    "print('Best Val accuracy: ' + str(max(LeakyReLU[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving Accuracy - Hidden 128\n",
    "\n",
    ">Modifying the model's architecture is also worth considering. Increase the hidden representation size from 10 to 128 and repeat the grid search over the hyper parameters. This time, what modification achieved the highest validation accuracy?\n",
    "\n",
    "hidden layerのsize(neuronの数)を10から128に変更して、上記のmini grid searchを行いhyper parameterのチューニングを行う"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| hidden size        | batch size           | learning rate  | momentum | activation function for hidden layer | validation accuracy of best performing model| note           |\n",
    "|:------------------:|:--------------------:|:--------------:|:--------:|:------------------------------------:|:-------------------:|:--------------:|\n",
    "| 128                 | 32                   | 0.1            | 0        | ReLU                                 | 0.978275  | baseline model |\n",
    "| 128                 | 64                   | 0.1            | 0        | ReLU                                 | 0.977151  |  |\n",
    "| 128                 | 32                   | 0.01            | 0        | ReLU                                 | 0.955047  |  |\n",
    "| 128                 | 32                   | 0.1            | 0.9        | ReLU                                 | 0.967413  |  |\n",
    "| 128                 | 32                   | 0.1            | 0        | LeakyReLU                                 | 0.979278  | best perfomance model |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "modelの設計で、inputの`Linear()`の第二引数(output size)とoutputの同じく`Linear()`の第一引数(input size)を10 -> 128に変更。これでhidden layerのneuronを10から128に変更したことなるようだ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [torch.nn.Linear](https://pytorch.org/docs/stable/nn.html#linear)\n",
    "- [torch.nn.Sequential](https://pytorch.org/docs/stable/nn.html#sequential)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #################################\n",
    "    ## Model specification TODO\n",
    "    model = nn.Sequential(\n",
    "#               nn.Linear(784, 10),\n",
    "              nn.Linear(784, 128),\n",
    "              nn.ReLU(),\n",
    "#               nn.LeakyReLU(),\n",
    "#             　nn.Linear(10, 10),\n",
    "              nn.Linear(128, 10),\n",
    "            )\n",
    "    lr=0.1\n",
    "    momentum=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.897118, 0.948948, 0.965582, 0.973437, 0.978549, 0.982198,\n",
       "        0.985125, 0.987459, 0.989497, 0.991312],\n",
       "       [0.947694, 0.96641 , 0.970588, 0.974265, 0.975936, 0.975769,\n",
       "        0.976939, 0.977607, 0.977273, 0.978275]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "baseline model\n",
    "\"\"\"\n",
    "\n",
    "base = np.array([\n",
    "    [.897118, .948948, .965582, .973437, .978549, .982198, .985125, .987459, .989497, .991312],\n",
    "    [.947694, .966410, .970588, .974265, .975936, .975769, .976939, .977607, .977273, .978275],\n",
    "])\n",
    "base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== base ======\n",
      "Best Train accuracy: 0.991312\n",
      "Best Val accuracy: 0.978275\n"
     ]
    }
   ],
   "source": [
    "print('====== base ======')\n",
    "print('Best Train accuracy: ' + str(max(base[0])))\n",
    "print('Best Val accuracy: ' + str(max(base[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.875315, 0.930883, 0.947157, 0.922505, 0.965581, 0.970844,\n",
       "        0.9747  , 0.977906, 0.980242, 0.982077],\n",
       "       [0.93246 , 0.951277, 0.961862, 0.935316, 0.970934, 0.972614,\n",
       "        0.974966, 0.975974, 0.977151, 0.976647]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "hyper parameter: batch size 32 -> 64\n",
    "\"\"\"\n",
    "\n",
    "batch64 = np.array([\n",
    "    [.875315, .930883, .947157, .922505, .965581, .970844, .974700, .977906, .980242, .982077],\n",
    "    [.932460, .951277, .961862, .935316, .970934, .972614, .974966, .975974, .977151, .976647],\n",
    "])\n",
    "batch64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== batch64 ======\n",
      "Best Train accuracy: 0.982077\n",
      "Best Val accuracy: 0.977151\n"
     ]
    }
   ],
   "source": [
    "print('====== batch64 ======')\n",
    "print('Best Train accuracy: ' + str(max(batch64[0])))\n",
    "print('Best Val accuracy: ' + str(max(batch64[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.796792, 0.891449, 0.905361, 0.913493, 0.919458, 0.925626,\n",
       "        0.929961, 0.934721, 0.938259, 0.941483],\n",
       "       [0.90625 , 0.920622, 0.924465, 0.93115 , 0.936664, 0.940007,\n",
       "        0.944352, 0.948362, 0.952039, 0.955047]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "hyper parameter: learning rate 0.1 -> 0.01\n",
    "\"\"\"\n",
    "\n",
    "lr = np.array([\n",
    "    [0.796792, 0.891449, 0.905361, 0.913493, 0.919458, 0.925626, 0.929961, 0.934721, 0.938259, 0.941483],\n",
    "    [0.906250, 0.920622, 0.924465, 0.931150, 0.936664, 0.940007, 0.944352, 0.948362, 0.952039, 0.955047],\n",
    "])\n",
    "lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== learning rate ======\n",
      "Best Train accuracy: 0.941483\n",
      "Best Val accuracy: 0.955047\n"
     ]
    }
   ],
   "source": [
    "print('====== learning rate ======')\n",
    "print('Best Train accuracy: ' + str(max(lr[0])))\n",
    "print('Best Val accuracy: ' + str(max(lr[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.912882, 0.955005, 0.962304, 0.967064, 0.972084, 0.974474,\n",
       "        0.975937, 0.977253, 0.97929 , 0.979364],\n",
       "       [0.961898, 0.964238, 0.959392, 0.958389, 0.965408, 0.963235,\n",
       "        0.960729, 0.957553, 0.962233, 0.967413]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "hyper parameter: momentum 0 -> 0.9\n",
    "\"\"\"\n",
    "\n",
    "momentum = np.array([\n",
    "    [0.912882, 0.955005, 0.962304, 0.967064, 0.972084, 0.974474, 0.975937, 0.977253, 0.979290, 0.979364],\n",
    "    [0.961898, 0.964238, 0.959392, 0.958389, 0.965408, 0.963235, 0.960729, 0.957553, 0.962233, 0.967413],\n",
    "])\n",
    "momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== momentum ======\n",
      "Best Train accuracy: 0.979364\n",
      "Best Val accuracy: 0.967413\n"
     ]
    }
   ],
   "source": [
    "print('====== momentum ======')\n",
    "print('Best Train accuracy: ' + str(max(momentum[0])))\n",
    "print('Best Val accuracy: ' + str(max(momentum[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.897044, 0.948818, 0.965323, 0.973214, 0.978123, 0.981939,\n",
       "        0.984884, 0.987459, 0.989219, 0.990905],\n",
       "       [0.947861, 0.965575, 0.970421, 0.973262, 0.975769, 0.976604,\n",
       "        0.977607, 0.978108, 0.978443, 0.979278]])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "hyper parameter: ReLU -> LeakyReLU \n",
    "\"\"\"\n",
    "\n",
    "LeakyReLU = np.array([\n",
    "    [0.897044, 0.948818, 0.965323, 0.973214, 0.978123, 0.981939, 0.984884, 0.987459, 0.989219, 0.990905],\n",
    "    [0.947861, 0.965575, 0.970421, 0.973262, 0.975769, 0.976604, 0.977607, 0.978108, 0.978443, 0.979278],\n",
    "])\n",
    "LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== LeakyReLU ======\n",
      "Best Train accuracy: 0.990905\n",
      "Best Val accuracy: 0.979278\n"
     ]
    }
   ],
   "source": [
    "print('====== LeakyReLU ======')\n",
    "print('Best Train accuracy: ' + str(max(LeakyReLU[0])))\n",
    "print('Best Val accuracy: ' + str(max(LeakyReLU[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Convolutional Neural Networks\n",
    "\n",
    ">Next, we are going to apply convolutional neural networks to the same task. These networks have demonstrated great performance on many deep learning tasks, especially in computer vision.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Networks\n",
    "\n",
    ">We provide skeleton code part2-mnist/nnet_cnn.py which includes examples of some (not all) of the new layers you will need in this part. Using the PyTorch Documentation, complete the code to implement a convolutional neural network with following layers in order:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">- A convolutional layer with 32 filters of size  3×3 \n",
    ">- A ReLU nonlinearity\n",
    ">- A max pooling layer with size  2×2 \n",
    ">- A convolutional layer with 64 filters of size  3×3 \n",
    ">- A ReLU nonlinearity\n",
    ">- A max pooling layer with size  2×2 \n",
    ">- A flatten layer\n",
    ">- A fully connected layer with 128 neurons\n",
    ">- A dropout layer with drop probability 0.5\n",
    ">- A fully-connected layer with 10 neurons\n",
    "\n",
    "> Note: We are not using a softmax layer because it is already present in the loss: PyTorch's nn.CrossEntropyLoss combines nn.LogSoftMax with nn.NLLLoss.\n",
    "\n",
    ">Without GPU acceleration, you will likely find that this network takes quite a long time to train. For that reason, we don't expect you to actually train this network until convergence. Implementing the layers and verifying that you get approximately 93% training accuracy and 98% validation accuracy after one training epoch (this should take less than 10 minutes) is enough for this project. If you are curious, you can let the model train longer; if implemented correctly, your model should achieve >99% test accuracy after 10 epochs of training. If you have access to a CUDA compatible GPU, you could even try configuring PyTorch to use your GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[torch.nn.Conv2d](https://pytorch.org/docs/stable/nn.html#conv2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "## Model specification TODO\n",
    "model = nn.Sequential(\n",
    "          nn.Conv2d(1, 32, (3, 3)),\n",
    "          nn.ReLU(),\n",
    "          nn.MaxPool2d((2, 2)),\n",
    "        )\n",
    "##################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_6.86x)",
   "language": "python",
   "name": "conda_6.86x"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
